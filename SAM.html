

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-67835289-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-67835289-3');
</script>

<!-- ======================================================================= -->
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<style type="text/css">
  body {
    /* font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; */
    font-family: "Avenir", sans-serif;
    font-weight:300;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 1100px;
  }

  h1 {
    font-weight:300;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }

  #authors td {
    padding-bottom:5px;
    padding-top:30px;
  }
</style>


<html>
  <head>

  <title>Social Adaptive Module for Weakly-supervised Group Activity Recognition</title>
  <meta property="og:title" content="Social Adaptive Module for Weakly-supervised Group Activity Recognition " />
  <meta property="og:image" content="" />
  <meta property="og:image:width" content="3000" />
  <meta property="og:image:height" content="800" />
  </head>

  <body>
    <br><br><br><br>
          <center>
            <span style="font-size:36px">Social Adaptive Module for Weakly-supervised<br> Group Activity Recognition</span>
        
        <span style="font-size:24px"></span>
        <br><br>
      <table align=center width=1000px>
            <tr>
                    <td align=center width=150px>
              <center>
                <span style="font-size:24px"><a href="http://ruiyan1995.github.io">Rui Yan</a></span>
                </center>
                </td>
                    <td align=center width=150px>
              <center>
                <span style="font-size:24px"><a href="http://lingxixie.com/Home.html">Lingxi Xie</a></span>
                </center>
                </td>
                    <td align=center width=200px>
              <center>
                <span style="font-size:24px"><a href="https://imag-njust.net/jinhui-tang">Jinhui Tang</a></span>
                </center>
                </td>

                <td align=center width=200px>
                <center>
                <span style="font-size:24px"><a href="">Xiangbo Shu</a></span>
                </center>
                </td>

                <td align=center width=150px>
                <center>
                <span style="font-size:24px"><a href="">Qi Tian</a></span>
                </center>
                </td>
        </table>

          <table align=center width=650px>
            <tr>
              <td align=center width=150px>
                <center>
                  <span style="font-size:24px"><a href="https://arxiv.org/pdf/2007.09470"> [Paper]</a></span>
                  </center>
                  </td>
                <td align=center width=150px>
              <center>
                <span style="font-size:24px"><a href="https://github.com/ruiyan1995/Weakly-supervised-Group-Activiy-Recognition"> [Code (soon)]</a></span>
                </center>
                </td>
                <!-- <td align=center width=150px>
              <center>
                <span style="font-size:24px"><a href="https://www.youtube.com/watch?v=jzPGVHcQ87s&list=PLi_9VI6bR6PxrAIJAaVH6LAt3LYZEdlNZ&index=7&t=0s"> [Talk]</a></span>
                </center>
                </td> -->
                <!-- <td align=center width=150px>
              <center>
                <span style="font-size:24px"><a href=""> [Poster]</a></span>
                </center>
                </td> -->
               </tr>
            <tr>
        </table>
          </center>

<!--        <br><br>
      <hr> -->

        <br><br>
        <table align=center width=850px>
          <tr>
            <td width=400px>
              <center>
                        <img src = "img/SAM.png" width="100%"></img><br>
              </center>
            </td>
          </tr>


        </table>

          <br><br>

      <hr>

        <table align=center width=850px>
          <center><h1>Abstract</h1></center>
          <p style="text-align:justify";>
          This paper presents a new task named weakly-supervised
          group activity recognition (GAR) which differs from conventional GAR
          tasks in that only video-level labels are available, yet the important persons within each frame are not provided even in the training data. This
          eases us to collect and annotate a large-scale NBA dataset and thus
          raise new challenges to GAR. To mine useful information from weak supervision, we present a key insight that key instances are likely to be
          related to each other, and thus design a social adaptive module (SAM)
          to reason about key persons and frames from noisy data. Experiments
          show significant improvement on the NBA dataset as well as the popular
          volleyball dataset. In particular, our model trained on video-level annotation achieves comparable accuracy to prior algorithms which required
          strong labels.
          </p>
      </table>


        <br><br>
      </table>

      <!-- <hr> -->


        <!-- <table align=center width=550px> -->
        <!-- <table align=center width=425px>
      <center><h1>Results on Tracking Texture, Mask and Pose</h1></center>
          <tr>
              </tr>
        </table> -->
      <!-- <br> -->

      <!-- <br> -->
      <hr>

      <center>
<!--       <h1>Qualitative Results</h1>
      <table border="0" align="center" cellspacing="0" cellpadding="20">
          <td align="center" valign="middle">
          <iframe width="800" height="450" src="https://www.youtube.com/embed/R_Zae5N_hKw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </td>
      </table> -->
      </center>


      <!-- <hr> -->
        <!-- <table align=center width=550px> -->
        <table align=center width=800px>
      <center><h1>NBA Dataset (NUST-NBA181)</h1></center>
          <p style="text-align:justify";>
          We collect a total of 181 NBA game videos with a high resolution of 1920 X 1080. Then we divide each video into 6-second clips and sub-sample them to 12fps. Ultimately, there are a total of 9,172 video clips, each of which belongs to one of the 9 activities. We used 7,624 clips for training and 1,548 clips for testing. The train-test split of this dataset is performed at video level, rather than at frame level.<br>

          <br>
          <strong>Download links</strong>: <a href="https://pan.baidu.com/s/1BBcGVOeP7ousHA1Mzn77lg">Baidu</a> or <a href="">Google Drive</a>. Limited by the copyright, this dataset (~135GB) is available upon request and only for academic research. Please contact me for the password by email (<em>ruiyan _at_ njust.edu.cn<em>).
          <br>
          <br>
          <strong>Group Activity Class</strong>:
          <table border="1">
            <tr>
              <th>Group Activity</th>
              <th># clips (Train/Test)</th>
            </tr>
            <tr>
              <td>2p-succ.</td>
              <td>798/163</td>
            </tr>
            <tr>
              <td>2p-fail.-off.</td>
              <td>434/107</td>
            </tr>
            <tr>
              <td>2p-fail.-def.</td>
              <td>1316/234</td>
            </tr>
            <tr>
              <td>2p-layup-succ.</td>
              <td>822/172</td>
            </tr>
            <tr>
              <td>2p-layup-fail.-off.</td>
              <td>455/89</td>
            </tr>
            <tr>
              <td>2p-layup-fail.-def.</td>
              <td>702/157</td>
            </tr>
            <tr>
              <td>3p-succ.</td>
              <td>728/183</td>
            </tr>
            <tr>
              <td>3p-fail.-off.</td>
              <td>519/83</td>
            </tr>
            <tr>
              <td>3p-fail.-def.</td>
              <td>1850/360</td>
            </tr>
          </table>
          <br>
          <br>
          <strong>Train-test split</strong>: provided in the <strong>train_video_ids</strong> and <strong>test_video_ids</strong>
          <br>
          <br>
          <strong>Further information:</strong>
          <li>
            Inside each video directory, a set of directories corresponds to annotated clips (e.g. <strong>"NBA/21800909/17" represents "Video ID 21800909, Clip ID 17"</strong>. Each clip directory has <strong>72</strong> images (frames). We release high-resolution (1920 X 1080) images here, but only used the low-resolution version (224 X 224) in our experiments due to the limitation of computational resources.
          </li>

          <li>
            Each video directory has an <strong>annotations.txt</strong> file that contains annotations for the selected clips. Each annotation line in format: {Clip ID} {Group Activity Class}
          </li>
          <li>
            The <strong>detections.pkl</strong> file contains all possible bounding boxes for people in the scene, generated by Faster-RCNN pre-trained on the MS-COCO.
          </li>
          <li>
            <strong>This dataset is easy to be extended. You can collect more NBA videos from the Internet and play-by-play data from NBA's official website. If you are interested in this, please feel free to contact me.</strong>
          </li>
          
          </p>
          <span style="font-size:4pt"><a href=""><br></a>
          </span>
          </td>
                  </td>
              </tr>
        </table>
      <br>

<!--       <table align=center width=600px>
        <tr>
          <td><span style="font-size:14pt"><center>
            <a href="./figs/bibtex.txt">[Bibtex]</a>
                  </center></td>
              </tr>
        </table> -->

        <hr>

        <table align=center width=1100px>
          <tr>
                  <td width=400px>
            <left>
           
          <center><h1>Acknowledgements</h1></center>
          <p style="text-align:justify";>
          This work was supported by the National Key Research and Development Program of China under Grant 2018AAA0102002, the National Natural Science Foundation of China under Grants 61732007, 61702265, and 61932020.
          </p>
          <hr>
          <center><h1>Citation</h1></center>
          <p style="text-align:justify";>
          In case using our NBA dataset or wish to refer to the baseline results, please cite the following publications.<br>
          @article{yan2020social,<br>
                    title={Social Adaptive Module for Weakly-supervised Group Activity Recognition},<br>
                    author={Yan, Rui and Xie, Lingxi and Tang, Jinhui and Shu, Xiangbo and Tian, Qi},<br>
                    journal={arXiv preprint arXiv:2007.09470},<br>
                    year={2020}<br>
                  }<br>
          @article{tang2019coherence,<br>
                    title={Coherence constrained graph LSTM for group activity recognition},<br>
                    author={Tang, Jinhui and Shu, Xiangbo and Yan, Rui and Zhang, Liyan},<br>
                    journal={IEEE transactions on pattern analysis and machine intelligence},<br>
                    year={2019}<br>
                  }<br>
          @inproceedings{yan2018participation,<br>
                    title={Participation-contributed temporal dynamic model for group activity recognition},<br>
                    author={Yan, Rui and Tang, Jinhui and Shu, Xiangbo and Li, Zechao and Tian, Qi},<br>
                    booktitle={Proceedings of the 26th ACM international conference on Multimedia},<br>
                    pages={1292--1300},<br>
                    year={2018}<br>
                  }<br>
          </p>

        </table>
          <br><br>
        </table>

      </left>
    </td>
       </tr>
    </table>

    <br><br>


</body>
</html>
