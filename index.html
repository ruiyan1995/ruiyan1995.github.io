
 <!DOCTYPE html>

<html><head>
<title>Rui Yan</title>

<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">

<script src="https://code.jquery.com/jquery-3.1.1.slim.min.js" integrity="sha384-A7FZj7v+d/sdmMqp/nOQwliLvUsJfDHW+k9Omg/a/EheAdgtzNs3hpfag6Ed950n" crossorigin="anonymous"></script>

<style type="text/css">
 @import url("http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,300italic,600,600italic");


	body
	{
	font-family:"Roboto",Helvetica,Arial,sans-serif;font-size:16px;line-height:1.5;font-weight:300;
    	background-color : #CDCDCD;
	}
    	.content
	{
    		width : 900px;
    		padding : 25px 30px;
    		margin : 25px auto;
    		background-color : #fff;
    		box-shadow: 0px 0px 10px #999;
    		border-radius: 15px; 
	}	
	table
	{
		padding: 5px;
	}
	
	table.pub_table,td.pub_td1,td.pub_td2
	{
		padding: 8px;
		width: 850px;
        border-collapse: separate;
        border-spacing: 15px;
        margin-top: -5px;
	}

	td.pub_td1
	{
		width:50px;
	}
    td.pub_td1 img
    {
        height:120px;
        width: 160px;
    }
	
	div#container
	{
		margin-left: auto;
		margin-right: auto;
		width: 820px;
		text-align: left;
		position: relative;
		background-color: #FFF;
	}
	div#DocInfo
	{
		color: #1367a7;
		height: 158px;
	}
	h4,h3,h2,h1
	{
		color: #3B3B3B;
	}
	h2
	{
		font-size:130%;
	}
	p
	{
		color: #5B5B5B;
		margin-bottom: 50px;
	}
	p.caption
	{
		color: #9B9B9B;
		text-align: left;
		width: 600px;
	}
	p.caption2
	{
		color: #9B9B9B;
		text-align: left;
		width: 800px;
	}
	#header_img
	{
		position: absolute;
		top: 0px; right: 0px;
    }
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}

    #mit_logo {
        position: absolute;
        left: 646px;
        top: 14px;
        width: 200px;
        height: 20px;
    }
   
    table.pub_table tr {
        outline: thin dotted #666666;
    }
    .papericon {
        border-radius: 8px; 
        -moz-box-shadow: 3px 3px 6px #888;
        -webkit-box-shadow: 3px 3px 6px #888;
        box-shadow: 3px 3px 6px #888;
        width: 180px;
	margin-top:5px;
	margin-left:5px;
	margin-bottom:5px;
    }

     .papericon_blank {

        width: 160px;
	margin-top:5px;
	margin-left:5px;
	margin-bottom:5px;
    }

    .media {
	outline: thin dotted #666666;
 	margin-bottom: 15px;	
	margin-left:10px;
    }
    .media-body {
	margin-top:5px;
	padding-left:20px;
    }

.papers-selected h5, .papers-selected h4 { display : none; }
.papers-selected .publication { display : none; }
.paperhi-only { display : none; }
.papers-selected .paperhi { display : flex; }
.papers-selected .paperlo { display : none; }

.hidden>div {
	display:none;
}

.visible>div {
	display:block;
}
</style>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-23931362-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-23931362-2');
</script>

<script type="text/javascript">
    var myPix = new Array("image/bolei_20191122.jpg", "image/profile_illustration.png")
    function choosePic() {
        var randomNum = Math.floor(Math.random() * myPix.length);
        document.getElementById("myPicture").src = myPix[randomNum];
    };
</script>

<script>
$(document).ready(function() {
  $('.paperlo button').click(function() {
     $('.papers-container').addClass('papers-selected');
  });
  $('.paperhi button').click(function() {
     $('.papers-container').removeClass('papers-selected');
  });


	$('.text_container').addClass("hidden");

	$('.text_container').click(function() {
		var $this = $(this);

		if ($this.hasClass("hidden")) {
			$(this).removeClass("hidden").addClass("visible");
			$(this).removeClass("papericon");
		} else {
			$(this).removeClass("visible").addClass("hidden");
		}
	});


});
</script>

</head>


<body>
<div class="content">
	<div id="container">

	<table>
	<tbody><tr>
	<td><img id="myPicture" src="image/bolei_20191122.jpg" style="float:left; padding-right:20px" height="200px"></td>
	<script>choosePic();</script>
	<td>
	<div id="DocInfo">
		<h1>Bolei Zhou</h1>
        Assistant Professor<br>
	Department of Information Engineering, The Chinese University of Hong Kong<br>
        Office: Room 717, Ho Sin-Hang Engineering Building<br>
        Email: <img src="email_cuhk.png" height="20px"><br>
        <a href="CV_web.pdf">CV</a> &bull; <a href="https://scholar.google.com/citations?user=9D4aG8AAAAAJ&hl=en">Google Scholar</a> &bull; <a href="https://github.com/zhoubolei">Github</a> &bull; <a href="https://twitter.com/zhoubolei">Twitter</a> &bull; <a href="https://www.zhihu.com/people/zhou-bo-lei/answers">Zhihu</a><br>
	</div><br>
    <!--
    <div id="mit_logo">
        <a href="http://www.mit.edu"><img src="image/mit.gif" height="170px" class="papericon" /></a>
    </div>
    -->
	</td>
	</tr>
	</tbody></table>
	<br>


	<h2>Intro</h2>
    <ul>
        <li>My research is at the intersection of machine perception and decision, with a focus on enabling machines to sense and act in complex environments through learning interpretable and structural representations.</li> 
	<li>My previous work includes <a href="http://places2.csail.mit.edu/">Places</a>, <a href="https://groups.csail.mit.edu/vision/datasets/ADE20K/">ADE20K</a>, <a href="http://cnnlocalization.csail.mit.edu/">Class Activation Mapping(CAM)</a>, <a href="http://netdissect.csail.mit.edu/">Network Dissection</a>, <a href="http://relation.csail.mit.edu/">TRN</a>.</li>
	<!--  
	<li>Recent work on interpreting deep generative models: <a href="https://shenyujun.github.io/InterFaceGAN/">InterFaceGAN</a>, <a href="https://ceyuan.me/SemanticHierarchyEmerge/">Generative Hierarchy</a>, <a href="http://ganpaint.io/">GANPaint</a>.</li>
	<li>Recent work on machine decision: <a href="https://sites.google.com/view/neurips2019pchid">Policy Continuation</a>, <a href="https://sites.google.com/view/decision-module/">Driving Imitation</a>, <a href="https://view-parsing-network.github.io/">Cross-view Segmentation</a>.</li>
   
   <li> My representative work includes the large-scale scene benchmarks <a href="http://places2.csail.mit.edu">Places Database and Places-CNN</a>, <a href="http://groups.csail.mit.edu/vision/datasets/ADE20K/">ADE20K Dataset</a>, as well as neural network interpretation methods <a href="http://cnnlocalization.csail.mit.edu/">Class Activation Mapping (CAM)</a> and <a href="http://netdissect.csail.mit.edu/">Network Dissection</a>. Recently I investigate video scene understanding, with
     work <a href="http://relation.csail.mit.edu/">Temporal Relational Reasoning</a> and <a href="http://moments.csail.mit.edu/">Moments in Time</a>.</li> 
-->
    </ul>
	
    <h2>News</h2>
    <ul>
    	<li>2020/06/13: <a href="https://interpretablevision.github.io/">3rd Tutorial on Interpretable Machine Learning</a> is organized at CVPR'20 with videos available.</li>
    	<li>2020/04/27: <b><a href="talent.html">Multiple RA and PhD positions are available.</a></b></li>
    	<li>2020/04/14: <a href="https://genforce.github.io/">GenForce</a> is online, our new research initiative on generative modeling.</li>
    	<li>2020/04/14: <a href="https://decisionforce.github.io/TPN/">Temporal Pyramid Network(TPN)</a> achieves huge improvement over <a href="http://relation.csail.mit.edu/">TRN</a>, with code.</li>
    	<li>2020/03/31: For fun I initiated an educational channel <a href="https://github.com/zhoubolei/introRL">Intro to Reinforcement Learning</a> with videos and slides.</li>
    	<li>2020/03/31: Source codes of the CVPR2020 works <a href="https://genforce.github.io/interfacegan/">InterfaceGAN</a> and <a href="hhttps://genforce.github.io/mganprior/">mGANprior</a> are released. </li> 
    	<li>2019/12/03: Fortunate to be among <a href="https://www.innovatorsunder35.com/regions/asiapacific/">MIT Tech Review's Innovators Under 35 in Asia Pacific</a>.</li>
    	<!--
    	<li>2019/10/29: 1 paper on reinforcement learning is accepted to NeurIPS'19 as spotlight. See paper below. </li>
	<li>2019/10/28: <a href="https://interpretablevision.github.io/">2nd Tutorial on Interpretable Machine Learning for Computer Vision</a> went well. Slides released. </li>
	<li>2019/09/03: 3 papers are accepted to ICCV'19 (2 orals + 1 poster). See the papers below.</li>
	<li>2019/07/01: New <a href="publication/siggraph19_preprint_small.pdf">SIGGRAPH'19 paper</a> on semantic image manipulation. Try the <a href="http://ganpaint.io/demo/">live demo</a>!</li>
	
	<li>[2019/07/01] New arXiv preprint on <a href="https://view-parsing-network.github.io/">cross-view semantic segmentation</a>.</li>
		<li>[2019/04/26] Talks at <a href="http://ganocracy.csail.mit.edu/">MIT GANocracy Workshop</a>, <a href="http://bzhou.ie.cuhk.edu.hk/cvpr19_tutorial/">CVPR'19 Tutorial on Tectures, Objects and Scenes</a>, <a href="https://amlcvpr2019.github.io/">CVPR'19 Adversarial Machine Learning Workshop</a>, and <a href="https://lidchallenge.github.io/">CVPR'19 Learning from Imperfect Data (LID) workshop</a>.</li>
	

	<li>[2019/03/05] Welcome to <a href="https://explainai.net/">CVPR'19 Workshop on Explainable AI</a>.
	<li>[2019/01/09] I am teaching <a href="https://course.ie.cuhk.edu.hk/~ierg6130/">IERG6130 Course on Reinforcement Learning</a>.</li>
    
 	<li>[2018/09/14] <a href="http://relation.csail.mit.edu">Temporal Relation Network</a> is covered by <a href="http://news.mit.edu/2018/machine-learning-video-activity-recognition-0914">MIT News</a> as Today's Spotlight.</a>

        <li>[2018/07/03] The videos for CVPR'18 Tutorial on Interpretable Machine Learning are <a href="https://interpretablevision.github.io/">available</a>.</li>
    
    	<li>[2018/05/04] I defended my Ph.D. thesis. Defense talk titled Interpretable Representation Learning for Visual Intelligence is available on <a href="https://www.youtube.com/watch?v=J7Zz_33ZeJc">Youtube</a> or <a href="http://people.csail.mit.edu/bzhou/bolei_defense.mp4">Download</a>.</li>
        <li>[2018/04/09] PyTorch implementation of scene parsing networks trained on ADE20K is <a href="https://github.com/CSAILVision/semantic-segmentation-pytorch">released</a>.</a></li>
        <li>[2017/12/09] I will organize the <a href="https://interpretablevision.github.io/">Tutorial on Interpretable Machine Learning at CVPR'18</a>.</li>
        <li>[2017/12/03] <a href="http://moments.csail.mit.edu/">Moments in Time Dataset</a> with 1 million videos from 339 actions is online! </li>
        <li>[2017/12/03] Latest work on <a href="http://relation.csail.mit.edu/">temporal reasoning</a> in videos. Relation is all you need. </li>
        <li>[2017/12/02] I am invited as a panelist for the <a href="http://interpretable.ml/">NIPS'17 Interpretable Machine Learning Symposium</a>.</li>
        <li>[2017/11/15] <a href="http://cnnlocalization.csail.mit.edu/">Class Activation Mapping</a> is used to <a href="https://news.stanford.edu/2017/11/15/algorithm-outperforms-radiologists-diagnosing-pneumonia/?linkId=44774396&linkId=44811912">interpret lung disease diagnosis</a> by researchers at Stanford.</li>
        <li>[2017/09/04] <a href="http://places2.csail.mit.edu/demo.html">Demo of Places365-CNN</a> is updated, which could predict the scene categories, attributes, and the class activation map together. <a href="https://github.com/CSAILVision/places365/blob/master/run_placesCNN_unified.py">Source code in PyTorch</a> is available.</li>
        <li>[2017/07/06] An invited talk at <a href="https://2017.icml.cc/">ICML'17</a> <a href="http://icmlviz.github.io/">Workshop on Visualization for Deep Learning</a> about interpreting deep visual representation. Here is the <a href="http://people.csail.mit.edu/bzhou/ppt/presentation_ICML_workshop.pdf">slide</a>. </li>
        <li>[2017/07/01] <a href="http://news.mit.edu/2017/inner-workings-neural-networks-visual-data-0630">MIT News</a> and <a href="https://techcrunch.com/2017/06/30/mit-csail-research-offers-a-fully-automated-way-to-peer-inside-neural-nets/">Techcrunch</a> cover our Network Dissection work. </li>
        <li>[2017/06/20] I am organizing the <a href="https://places-coco2017.github.io/">Joint Workshop for COCO and Places Challenge at ICCV'17</a>.</li>
        -->
    </ul>


<div class="papers-container papers-selected"> 
	<h5 class="paperlo">All Publications<button type="button" class="ml-3 btn btn-light"> Show selected</button></h5>
	<h5 class="paperhi paperhi-only">Selected Publications<button type="button" class="ml-3 btn btn-light"> Show all</button></h5>
	<h5 class="pt-2 pb-1">2020</h5>	

	<div class="publication media paperhi">
           <img src="image/cover_dice.png" class="papericon">
           <div class="media-body"><b>Non-local Policy Optimization via Diversity-regularized Collaborative Exploration</b><br>
           Zhenghao Peng,  Hao Sun,  Bolei Zhou<br>arXiv:2006.07781<br>
           [<a href="https://arxiv.org/pdf/2006.07781.pdf">PDF</a>][<a href="https://decisionforce.github.io/DiCE/">Webpage</a>][<a href="https://github.com/decisionforce/DiCE">Code</a>]
	</div></div>

	<div class="publication media paperhi">
           <img src="image/cover_higan.gif" class="papericon">
           <div class="media-body"><b>Semantic Hierarchy Emerges in Deep Generative Representations for Scene Synthesis</b><br>
           Ceyuan Yang*,  Yujun Shen*,  Bolei Zhou<br>arXiv:1911.09267<br>
           [<a href="https://arxiv.org/pdf/1911.09267.pdf">PDF</a>][<a href="https://genforce.github.io/higan/">Webpage</a>][<a href="https://github.com/genforce/higan">Code</a>]
	</div></div>

	<div class="publication media paperhi">
           <img src="image/cover_VPN.png" class="papericon">
           <div class="media-body"><b>Cross-view Semantic Segmentation for Sensing Surroundings</b><br>
           Bowen Pan, Jiankai Sun, Ho Yin Tiga Leung, Alex Andonian, Bolei Zhou<br>IEEE Robotics and Automation Letters (RA-L) and IROS 2020<br>
           [<a href="https://view-parsing-network.github.io/View_Parsing_Network_files/vpn_iros.pdf">PDF</a>][<a href="https://view-parsing-network.github.io/">Webpage</a>][<a href="https://github.com/pbw-Berwin/View-Parsing-Network">Code</a>]
	</div></div>

	<div class="publication media paperhi">
           <img src="image/cover_NPS.png" class="papericon">
           <div class="media-body"><b>Novel Policy Seeking with Constrained Optimization</b><br>
           Hao Sun, Zhenghao Peng, Bo Dai, Jian Guo, Dahua Lin, Bolei Zhou<br>arXiv:2005.10696<br>
           [<a href="https://arxiv.org/pdf/2005.10696.pdf">PDF</a>]
	</div></div>

	<div class="publication media paperhi">
           <img src="image/icon_idinv.png" class="papericon">
           <div class="media-body"><b>In-Domain GAN Inversion for Real Image Editing</b><br>
           Jiapeng Zhu, Yujun Shen, Deli Zhao, Bolei Zhou<br>ECCV 2020<br>
           [<a href="https://arxiv.org/pdf/2004.00049.pdf">PDF</a>][<a href="https://genforce.github.io/idinvert/">Webpage</a>][<a href="https://github.com/genforce/idinvert">Code</a>]
	</div></div>

		<div class="publication media paperhi">
           <img src="image/icon_espn.png" class="papericon">
           <div class="media-body"><b>Evolutionary Stochastic Policy Distillation</b><br>
           Hao Sun, Xinyu Pan, Bo Dai, Dahua Lin, Bolei Zhou<br>arXiv:2004.12909<br>
           [<a href="https://arxiv.org/pdf/2004.12909.pdf">PDF</a>][<a href="https://github.com/decisionforce/ESPD">Code</a>]
	</div></div>

	<div class="publication media paperhi">
           <img src="image/icon_tpn.png" class="papericon">
           <div class="media-body"><b>Temporal Pyramid Network for Action Recognition</b><br>
           Ceyuan Yang*, Yinghao Xu*, Jianping Shi, Bo Dai, Bolei Zhou<br>CVPR 2020<br>
           [<a href="https://arxiv.org/pdf/2004.03548.pdf">PDF</a>][<a href="https://decisionforce.github.io/TPN/">Webpage</a>][<a href="https://github.com/decisionforce/TPN">Code</a>]
	</div></div>

	<div class="publication media paperhi">
           <img src="image/icon_interfacegan.png" class="papericon">
           <div class="media-body"><b>Interpreting Latent Space of GANs for Semantic Face Editing</b><br>
           Yujun Shen, Jinjin Gu, Xiaoou Tang, Bolei Zhou<br>CVPR 2020<br>[<a href="https://arxiv.org/pdf/1907.10786.pdf">PDF</a>][<a href="https://genforce.github.io/interfacegan/">Webpage</a>]
	</div></div>

	<div class="publication media paperhi">
           <img src="image/icon_imageprocessing.png" class="papericon">
           <div class="media-body"><b>Image Processing Using Multi-Code GAN Prior</b><br>
           Jinjin Gu, Yujun Shen, Bolei Zhou<br>CVPR 2020<br>[<a href="https://arxiv.org/pdf/1912.07116.pdf">PDF</a>][<a href="https://genforce.github.io/mganprior/">Webpage</a>]
	</div></div>

	<div class="publication media paperhi">
           <img src="image/icon_motionprediction.png" class="papericon">
           <div class="media-body"><b>TPNet: Trajectory Proposal Network for Motion Prediction</b><br>
           Liangji Fang, Qinhong Jiang, Jianping Shi, Bolei Zhou.<br>CVPR 2020<br>
           [<a href="https://arxiv.org/pdf/2004.12255.pdf">PDF</a>][<a href="https://decisionforce.github.io/TPNet/">Webpage</a>]
	</div></div>

	<div class="publication media paperhi">
           <img src="image/icon_local.png" class="papericon">
           <div class="media-body"><b>A Local-to-Global Approach to Multi-modal Movie Scene Segmentation</b><br>
           Anyi Rao, Linning Xu, Yu Xiong, Guodong Xu, Qingqiu Huang, Bolei Zhou, Dahua Lin<br>CVPR 2020<br>
           [<a href="https://arxiv.org/pdf/2004.02678.pdf">PDF</a>][<a href="https://anyirao.com/projects/SceneSeg.html">Webpage</a>][<a href="https://github.com/AnyiRao/SceneSeg">Code</a>]
	</div></div>

	<div class="publication media">
           <img src="image/icon_transmomo.png" class="papericon">
           <div class="media-body"><b>Video Motion Retargeting via Invariance-Driven Unsupervised Representation Disentanglement</b><br>
           Zhuoqian Yang, Wentao Zhu, Wayne Wu, Chen Qian, Qiang Zhou, Bolei Zhou, Chen Change Loy<br>CVPR 2020<br>
           [<a href="https://arxiv.org/pdf/2003.14401.pdf">PDF</a>][<a href="https://yzhq97.github.io/transmomo/">Webpage</a>]
	</div></div>


	<div class="publication media">
           <img src="" class="papericon_blank">
           <div class="media-body"><b>Every Frame Counts: Joint Learning of Video Segmentation and Optical Flow</b><br>
           Mingyu Ding, Zhe Wang, Bolei Zhou, Jianping Shi, Zhiwu Lu, Ping Luo<br>AAAI 2020<br>[<a href="https://arxiv.org/pdf/1911.12739.pdf">PDF</a>]
	</div></div>

	<h5 class="pt-2 pb-1">2019</h5>	
	<div class="publication media paperhi">
           <img src="image/cover_neurips19.png" class="papericon">
           <div class="media-body"><b>Policy Continuation with Hindsight Inverse Dynamics</b><br>
           Hao Sun, Zhizhong Li, Xiaotong Liu, Dahua Lin, Bolei Zhou<br>NeurIPS 2019, Spotlight<br>[<a href="publication/neurips19_PCHID.pdf">PDF</a>][<a href="https://sites.google.com/view/neurips2019pchid">Webpage</a>]
	</div></div>


	<div class="publication media">
           <img src="image/snapshot_iccv19_see_gan.png" class="papericon">
           <div class="media-body"><b>Seeing What a GAN Cannot Generate</b><br>
           David Bau, Jun-Yan Zhu, Jonas Wulff, William Peebles, Hendrik Strobelt, Bolei Zhou, Antonio Torralba<br>ICCV 2019, Oral<br>[<a href="publication/iccv19_see_gan.pdf">PDF</a>][<a href="http://ganseeing.csail.mit.edu">Webpage</a>]
	</div></div>

	<div class="publication media">
           <img src="image/snapshot_iccv19_movie_synopse.png" class="papericon">
           <div class="media-body"><b>A Graph-Based Framework to Bridge Movies and Synopses</b><br>
           Yu Xiong, Qingqiu Huang, Lingfen Guo, Hang Zhou, Bolei Zhou, Dahua Lin.<br>ICCV 2019, Oral<br>[<a href="">PDF</a>]
	</div></div>

	<div class="publication media">
           <img src="image/snapshot_iccv19_hoi_dual.png" class="papericon">
           <div class="media-body"><b>Reasoning About Human-Object Interactions Through Dual Attention Networks</b><br>
           Tete Xiao, Quanfu Fan, Dan Gutfreund, Mathew Monfort, Aude Oliva, Bolei Zhou<br>ICCV 2019<br>[<a href="publication/iccv19_hoi_dual.pdf">PDF</a>][<a href="https://dual-attention-network.github.io/">Webpage</a>]
	</div></div>

	<div class="publication media">
           <img src="image/siggraph19_teaser.png" class="papericon">
           <div class="media-body"><b>Semantic Photo Manipulation with a Generative Image Prior</b><br>
           David Bau, Hendrik Strobelt, William Peebles, Jonas Wulff, Bolei Zhou, Jun-Yan Zhu, Antonio Torralba<br>SIGGRAPH 2019<br>[<a href="publication/siggraph19_preprint_small.pdf">PDF</a>][<a href="http://ganpaint.io/">Webpage</a>][<a href="http://ganpaint.io/demo/?project=church">Live Demo</a>][<a href="http://news.mit.edu/2019/teaching-artificial-intelligence-to-create-more-common-sense-visuals-0701">MIT News</a>]
	</div></div>

	<div class="publication media">
           <img src="image/cvpr19_videoinpainting.png" class="papericon">
           <div class="media-body"><b>Deep Flow-Guided Video Inpainting</b><br>
           Rui Xu, Xiaoxiao Li, Bolei Zhou, Chen Change Loy<br>CVPR 2019<br>[<a href="https://arxiv.org/pdf/1905.02884.pdf">PDF</a>][<a href="https://nbei.github.io/video-inpainting.html">Webpage</a>][<a href="https://github.com/nbei/Deep-Flow-Guided-Video-Inpainting">Code</a>]
	</div></div>

	<div class="publication media">
           <img src="image/cvpr19_drivingstereo.png" class="papericon">
           <div class="media-body"><b>DrivingStereo: A large-scale dataset for stereo matching in autonomous driving scenarios.</b><br>
           Guorun Yang*, Xiao Song*, Chaoqin Huang, Zhidong Deng, Jianping Shi, Bolei Zhou.<br>CVPR 2019<br>[<a href="publication/cvpr19_drivingstereo.pdf">PDF</a>][<a href="https://drivingstereo-dataset.github.io/">Dataset</a>]
	</div></div>

	<div class="publication media">
           <img src="image/iclr19_teaser.png" class="papericon">
           <div class="media-body"><b>GAN Dissection: Visualizing and Understanding Generative Adversarial Networks.</b><br>David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B. Tenenbaum, William T. Freeman, Antonio Torralba.<br>ICLR 2019.<br>[<a href="publication/iclr19_gandissection.pdf">PDF</a>][<a href="http://gandissect.csail.mit.edu/">Webpage</a>][<a href="https://github.com/CSAILVision/GANDissect">Code</a>]
	</div></div>


	<div class="publication media">
           <img src="image/rsos.png" class="papericon">
           <div class="media-body"><b>Discovering place-informative scenes and objects using social media photos.</b><br>
           Fan Zhang, Bolei Zhou, Carlo Ratti, Yu Liu.<br>Royal Society Open Science, 2019<br>[<a href="publication/rsos.181375.pdf">PDF</a>]
	</div></div>

	<div class="publication media">
           <img src="image/landscape.png" class="papericon">
           <div class="media-body"><b>Measuring human perceptions of a large-scale urban region using machine learning.</b><br>
           Fan Zhang, Bolei Zhou, Liu Liu, Yu Liu, Helene Fung, Hui Lin, Carlo Ratti.<br>Landscape and Urban Planning, 2018<br>[<a href="publication/landscape_urbanplanning.pdf">PDF</a>]
	</div></div>



	<div class="publication media">
           <img src="image/cover_moments.png" class="papericon">
           <div class="media-body"><b>Moments in Time Dataset: one million videos for event understanding.</b><br>
           Mathew Monfort, Alex Andonian, Bolei Zhou, Sarah Adel Bargal, Tom Yan, Kandan Ramakrishnan, Lisa Brown, Quanfu Fan, Dan Gutfreund, Carl Vondrick, Aude Oliva.<br>IEEE Transaction on Pattern Analysis and Machine Intelligence, March 2019.<br>[<a href="https://arxiv.org/pdf/1801.03150.pdf">PDF</a>][<a href="http://moments.csail.mit.edu/">Website</a>][<a href="https://github.com/metalbubble/moments_models">Code+Model</a>]
	</div></div>

	<h5 class="pt-2 pb-1">2018</h5>	
	<div class="publication media">
           <img src="image/cover_sceneparsing.jpg" class="papericon">
           <div class="media-body"><b>Semantic Understanding of Scenes through ADE20K Dataset.</b><br>
	   Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso and Antonio Torralba.<br>International Journal on Computer Vision (IJCV), 2018. <br>[<a href="publication/ADE20K_IJCV.pdf" target="_blank">PDF</a>][<a href="http://groups.csail.mit.edu/vision/datasets/ADE20K/" target="_blank">Dataset</a>][<a href="https://github.com/CSAILVision/semantic-segmentation-pytorch/" target="_blank">Pretrained Models</a>][<a href="http://sceneparsing.csail.mit.edu">Benchmark Page</a>][<a href="http://scenesegmentation.csail.mit.edu" target="_blank">Demo</a>]
	   </div>
	</div>

	<div class="publication media">	
           <img src="image/cover_unitablation.png" class="papericon"></td>
           <div class="media-body"> <b>Revisiting the Importance of Individual Units in CNNs via Ablation.</b><br>
	   Bolei Zhou, Yiyou Sun, David Bau, and Antonio Torralba<br>arXiv:1806.02891, 2018.<br>[<a href="https://arxiv.org/pdf/1806.02891.pdf">arXiv</a>]
	   </div>	
	</div>
       
	<div class="publication media"> 
           <img src="image/cover_trn_stop.gif" class="papericon">
           <div class="media-body">
			<strong>Temporal Relational Reasoning in Videos.</strong><br>
           Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Torralba<br>ECCV 2018.<br>[<a href="publication/eccv18-TRN.pdf">PDF</a>][<a href="https://arxiv.org/pdf/1711.08496.pdf">arXiv</a>][<a href="http://relation.csail.mit.edu">Webpage</a>][<a href="https://www.youtube.com/watch?v=D42erLb42_k">Demo Video</a>][<a
               href="https://github.com/metalbubble/TRN-pytorch">Code</a>][<a href="http://news.mit.edu/2018/machine-learning-video-activity-recognition-0914">MIT News</a>]
	</div></div>

	<div class="publication media">
           <img src="image/cover_ibd.png" class="papericon">
           <div class="media-body"><strong>Interpretable Basis Decomposition for Visual Explanation.</strong><br>
           Bolei Zhou*, Yiyou Sun*, David Bau*, Antonio Torralba.<br>
           ECCV 2018.<br>[<a href="publication/eccv18-IBD.pdf">PDF</a>][<a href="https://github.com/metalbubble/IBD">Code</a>]
	</div></div>

	<div class="publication media">
           <img src="image/cover_upernet.png" class="papericon">
           <div class="media-body">
           <strong>Unified Perceptual Parsing for Scene Understanding.</strong><br>Tete Xiao*, Yingcheng Liu*, Bolei Zhou*, Yuning Jiang, Jian Sun<br>
           ECCV 2018.<br>[<a href="publication/eccv18-segment.pdf">PDF</a>][<a href="https://github.com/CSAILVision/unifiedparsing">Code & Data</a>]
	</div></div>
	
	<div class="publication media">
           <img src="image/cover_intrinsic.png" class="papericon">
           <div class="media-body">
           <strong>Single Image Intrinsic Decomposition without a Single Intrinsic Image.</strong><br>
			Wei-Chiu Ma, Hang Chu, Bolei Zhou, Raquel Urtasun, Antonio Torralba.
           <br>ECCV 2018.<br>[<a href="https://chuhang.github.io/files/publications/ECCV_18.pdf">PDF</a>]
	</div></div>
 
	<div class="publication media">
           <img src="image/cover_factorcnn.png" class="papericon">
           <div class="media-body">
           <strong>Factorizable Net: An Efficient Subgraph based Framework for Scene Graph Generation.</strong><br>
			Yikang Li, Wanli Ouyang, Bolei Zhou, Yawen Cui, Jianping Shi, Xiaogang Wang.<br>
           ECCV 2018.<br>[<a href="https://arxiv.org/pdf/1806.11538.pdf">PDF</a>]
	</div></div>
           
	<div class="publication media">
	  <img src="image/cover_iros18.png" class="papericon">
           <div class="media-body"><strong>Real-Time Object Pose Estimation with Pose Interpreter Networks.</strong><br>
          Jimmy Wu, Bolei Zhou, Rebecca Russell, Vincent Kee, Syler Wagner, Mitchell Hebert, Antonio Torralba, and David M.S. Johnson<br>IROS 2018.<br>[<a href="publication/iros18-pose.pdf">PDF</a>][<a href="https://github.com/jimmyyhwu/pose-interpreter-networks">Code</a>][<a href="https://www.youtube.com/watch?v=9QBw1NCOOR0">Video</a>]</td>
	</div></div>

	<div class="publication media"> 
           <img src="image/cover_thesis.png" class="papericon">
           <div class="media-body"><b>Interpretable Representation Learning for Visual Intelligence.</b><br>
           Bolei Zhou<br>PhD thesis submitted to MIT EECS, May 17, 2018.<br>Committee: Antonio Torralba, Aude Oliva, Bill Freeman.<br>[<a href="publication/thesis.pdf">PDF</a>][<a href="https://www.youtube.com/watch?v=J7Zz_33ZeJc">Defense Talk</a>]
	</div></div>

	<div class="publication media">
           <img src="image/cover_deepminer.png" class="papericon">
           <div class="media-body"><b>DeepMiner: Discovering Interpretable Representations for Mammogram Classification and Explanation.</b><br>
           Jimmy Wu, Bolei Zhou, Diondra Peck, Scott Hsieh, Vandana Dialani, Vasilis Syrgkanis, Lester Mackey, and Genevieve Patterson<br>arXiv:1805.12323, 2018.<br>[<a href="https://arxiv.org/pdf/1805.12323.pdf">arXiv</a>]
	</div></div>

	<div class="publication media">
           <img src="image/SPIE.png" class="papericon">
           <div class="media-body"><b>Expert identification of visual primitives used by CNNs during mammogram classification.</b><br>
           Jimmy Wu, Diondra Peck, Scott Hsieh, Vandana Dialani, Constance D. Lehman, Bolei Zhou, Vasilis Syrgkanis, Lester Mackey, and Genevieve Patterson<br>SPIE Medical Imaging, 2018.<br>[<a href="publication/SPIE18.pdf">PDF</a>]
	</div></div>

	<div class="publication media">
           <img src="image/cover_vqavqg.png" class="papericon">
           <div class="media-body"><b>Visual Question Generation as Dual Task of Visual Question Answering.</b><br>
           Yikang Li, Nan Duan, Bolei Zhou, Xiao Chu, Wanli Ouyang, and Xiaogang Wang<br>CVPR 2018, <b>spotlight</b>.<br>[<a href="https://arxiv.org/pdf/1709.07192.pdf">arXiv</a>][<a href="http://cvboy.com/publication/cvpr2018_iqan/">Webpage</a>][<a href="https://github.com/yikang-li/iQAN">Code</a>]
	</div></div>

	<div class="publication media">
           <img src="image/RRM.png" class="papericon">
           <div class="media-body"><b>Recurrent Residual Module for Fast Inference in Videos.</b><br>
           Bowen Pan, Wuwei Lin, Xiaolin Fang, Chaoqin Huang, Bolei Zhou, Cewu Lu<br>CVPR 2018.<br>[<a href="https://arxiv.org/pdf/1802.09723.pdf">arXiv</a>]
	</div></div>


	<div class="publication media">
           <img src="image/tpami_interpretation_screen.png" class="papericon">
           <div class="media-body"><strong>Interpreting Deep Visual Representations via Network Dissection.</strong><br>
           Bolei Zhou*, David Bau*, Aude Oliva, and Antonio Torralba.<br>IEEE Transactions on Pattern Analysis and Machine Intelligence, June 2018. *-indicates equal contributions <br>[<a href="https://arxiv.org/pdf/1711.05611.pdf" target="_blank">arXiv</a>][<a href="http://netdissect.csail.mit.edu" target="_blank">Webpage</a>][<a href="https://github.com/CSAILVision/NetDissect">Code</a>]
	</div></div>

	<h5 class="pt-2 pb-1">2017</h5>
	<div class="publication media">
           <img src="image/cover_places2.png" class="papericon">
           <div class="media-body"><b>Places: A 10 Million Image Database for Scene Recognition.</b><br>
           Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba.<br>IEEE Transactions on Pattern Analysis and Machine Intelligence, July 2017. <br>[<a href="http://places2.csail.mit.edu/PAMI_places.pdf" target="_blank">PDF</a>][<a href="http://places2.csail.mit.edu" target="_blank">Places2 Dataset</a>][<a href="http://places2.csail.mit.edu/challenge.html" target="_blank">Challenge Page</a>][<a href="https://github.com/metalbubble/places365" target="_blank">Places365 CNN models</a>][<a href="http://places2.csail.mit.edu/demo.html" target="_blank">Demo</a>]
	</div></div>

	<div class="publication media">
           <img src="image/cover_scenegraph.png" class="papericon">
           <div class="media-body"><b>Scene Graph Generation from Objects, Phrases and Region Captions.</b><br>
           Yikang Li, Wanli Ouyang, Bolei Zhou, Kun Wang, and Xiaogang Wang<br>ICCV 2017.<br>[<a href="publication/ICCV_scenegraph.pdf">PDF</a>][<a href="https://github.com/yikang-li/MSDN">Code</a>]
	</div></div>
 
	<div class="publication media">
           <img src="image/cover_openvoc.jpg" class="papericon">
           <div class="media-body"><b>Open Vocabulary Scene Parsing.</b><br>
           Hang Zhao, Xavier Puig, Bolei Zhou, Sanja Fidler, and Antonio Torralba<br>ICCV 2017.<br>[<a href="publication/ICCV_openvoc.pdf">PDF</a>][<a href="https://arxiv.org/pdf/1703.08769.pdf">arXiv</a>][<a href="http://sceneparsing.csail.mit.edu/openvoc/">Webpage</a>]
	</div></div>
    
	<div class="publication media">
           <img src="image/cover_sceneparsing_cvpr2017.png" class="papericon">
           <div class="media-body"><b>Scene Parsing through ADE20K Dataset.</b><br>
           Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso and Antonio Torralba.<br>CVPR 2017. <br>[<a href="publication/scene-parse-camera-ready.pdf">PDF</a>][<a href="http://groups.csail.mit.edu/vision/datasets/ADE20K/" target="_blank">Dataset</a>][<a href="http://sceneparsing.csail.mit.edu">Benchmark Page</a>][<a href="http://sceneparsing.csail.mit.edu/index_challenge.html" target="_blank">Challenge Page</a>][<a href="https://github.com/CSAILVision/sceneparsing" target="_blank">Toolkit&Code</a>][<a href="http://scenesegmentation.csail.mit.edu" target="_blank">Demo</a>]
	</div></div>

	<div class="publication media">
           <img src="image/cover_interpretability_cvpr2017.png" class="papericon">
           <div class="media-body"><b>Network Dissection: Quantifying Interpretability of Deep Visual Representations.</b><br>
           David Bau*, Bolei Zhou*, Aditya Khosla, Aude Oliva, and Antonio Torralba.<br>CVPR 2017. as <b>oral</b>. *-indicates equal contribution. <br>[<a href="http://netdissect.csail.mit.edu/final-network-dissection.pdf">PDF</a>][<a href="https://arxiv.org/pdf/1704.05796.pdf">arXiv</a>][<a href="http://netdissect.csail.mit.edu/">webpage</a>][<a href="">code</a>][<a href="https://www.youtube.com/watch?v=Xy6RcjXMa2c">Talk Video</a>]<br>
	</div></div>
 	
	<div class="publication media">
           <img src="image/cover_person_cvpr2017.png" class="papericon">
           <div class="media-body"><b>Person Search with Natural Language Description.</b><br>
           Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu Yue, and Xiaogang Wang.<br>CVPR 2017. <br>[<a href="https://arxiv.org/pdf/1702.05729.pdf">PDF</a>][<a href="http://xiaotong.me/static/projects/person-search-language/dataset.html">Dataset</a>]
	</div></div>

	<div class="publication media">
           <img src="image/cover_segicp.png" class="papericon">
           <div class="media-body"><b>SegICP: Integrated Deep Semantic Segmentation and Pose Estimation.</b><br>
           J. Wong, V. Kee, T. Le, S.Wagner, G. Mariottini, A. Schneider, L. Hamilton, R. Chiaplkatty, M. Herbert, D. Johnson<br> J. Wu, B. Zhou, and A. Torralba.<br>IROS 2017, Oral<br>[<a href="https://arxiv.org/pdf/1703.01661.pdf">PDF</a>]
	</div></div>
   
	<h5 class="pt-2 pb-1">2016</h5>
	<div class="publication media">
           <img src="image/cover_CAM.jpg" class="papericon">
           <div class="media-body"><b>Learning Deep Features for Discriminative Localization.</b><br>
           Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba<br>CVPR 2016.<br>[<a href="publication/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf" target="_blank">PDF</a>] [<a href="http://arxiv.org/pdf/1512.04150.pdf" target="_blank">arXiv</a>][<a href="http://cnnlocalization.csail.mit.edu" target="_blank">Project Page</a>][<a href="https://www.youtube.com/watch?v=fZvOy0VXWAI" target="_blank">Video of CNN shifting its attention</a>]
	</div></div>

	<div class="publication media">
           <img src="image/cover_zi.png" class="papericon">
           <div class="media-body"><b>Optimization as Estimation with Gaussian Processes in Bandit Settings.</b><br>
           Zi Wang, Bolei Zhou, Stephanie Jegelka<br>AISTATS 2016, Oral.<br>[<a href="http://arxiv.org/abs/1510.06423" target="_blank">PDF</a>][<a href="http://zi-wang.com/gp-est/" target="_blank">Project</a>][<a href="https://github.com/zi-w/GP-EST" target="_blank">Code</a>]
	</div></div>


	<h5 class="pt-2 pb-1">2015</h5>
	<div class="publication media">	
           <img src="image/cover_intraclass.png" class="papericon">
           <div class="media-body"><b>Understanding Intra-Class Knowledge inside CNN.</b><br>
           Donglai Wei, Bolei Zhou, Antonio Torralba, William Freeman<br>arXiv:1507.02379, 2015. <br>[<a href="http://arxiv.org/pdf/1507.02379.pdf" target="_blank">PDF</a>][<a href="http://vision03.csail.mit.edu/cnn_art/index.html" target="_blank">Page</a>][<a href="https://github.com/donglaiw/mNeuron/" target="_blank">Code</a>]
	</div></div>

	<div class="publication media">
           <img src="image/cover_vqa.jpg" class="papericon">
           <div class="media-body"><b>Simple Baseline for Visual Question Answering.</b><br>
           Bolei Zhou, Yuandong Tian, Sainbar Suhkbaatar, Arthur Szlam, Rob Fergus<br>arXiv:1512.02167, 2015. <br>[<a href="http://arxiv.org/pdf/1512.02167.pdf" target="_blank">PDF</a>][<a href="http://visualqa.csail.mit.edu" target="_blank">Demo</a>][<a href="https://github.com/metalbubble/VQAbaseline/" target="_blank">Code</a>]
	</div></div>

	<div class="publication media">
           <img src="image/cover_visualization.jpg" class="papericon">
           <div class="media-body"><b>Object Detectors Emerge in Deep Scene CNNs.</b><br>
           Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba<br>ICLR 2015, Oral.<br>[<a href="http://arxiv.org/pdf/1412.6856.pdf" target="_blank">PDF</a>][<a href="http://places.csail.mit.edu" target="_blank">Project Page</a>][<a href="http://places.csail.mit.edu/visualization/" target="_blank">More Visualization</a>][<a href="https://github.com/metalbubble/unitvisseg">Code</a>]
	</div></div>

	<div class="publication media">
           <img src="image/cover_concept.jpg" class="papericon" >
           <div class="media-body"><b>ConceptLearner: Discovering Visual Concepts from Weakly Labeled Image Collections.</b><br>
           Bolei Zhou, Vignesh Jagadeesh, and Robinson Piramuthu<br>CVPR 2015.<br>[<a href="http://conceptlearner.csail.mit.edu/conceptlearner_final.pdf" target="_blank">PDF</a>][<a href="http://conceptlearner.csail.mit.edu/" target="_blank">Project Page & Demo</a>]
	</div></div>

	<h5 class="pt-2 pb-1">2014</h5>
	<div class="publication media">
           <img src="image/cover_nips2014.jpg" class="papericon">
           <div class="media-body"><b>Learning Deep Features for Scene Recognition using Places Database.</b><br>
           Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva<br>NIPS 2014, Spotlight<br>[<a href="http://places.csail.mit.edu/places_NIPS14.pdf" target="_blank">PDF</a>][<a href="http://places.csail.mit.edu" target="_blank">Project Page</a>][<a href="http://places2.csail.mit.edu/demo.html" target="_blank">Demo</a>]
	</div></div>

	<div class="publication media">
	   <img src="image/boston_perception.jpg" class="papericon">
           <div class="media-body"><b>Recognizing City Identity via Attribute Analysis of Geo-tagged Images.</b><br>
	   Bolei Zhou, Liu Liu, Aude Oliva and Antonio Torralba<br>ECCV 2014<br>[<a href="project/eccv2014/ECCV14_cityperception.pdf" target="_blank">PDF</a>][<a href="http://cityimage.csail.mit.edu" target="_blank">Project Page</a>]<br>
        Liu Liu, Bolei Zhou, Jinhua Zhao, Brent D. Ryan<br><b>C-IMAGE: City Cognitive Mapping through Geo-tagged Photos</b><br>GeoJournal, Springer, 2016.<br>[<a href="project/eccv2014/c-image.pdf" target="_blank">PDF</a>]<br>
	</div></div>

	<div class="publication media">
	    <a href="http://mmlab.ie.cuhk.edu.hk/project/collectiveness/"><img src="image/cover_cvpr2013.jpg" class="papericon"></a>
           <div class="media-body"><b>Measuring Crowd Collectiveness.</b><br>
		Bolei Zhou, Xiaoou Tang, Hepeng Zhang and Xiaogang Wang<br> IEEE transaction on Pattern Analysis and Machine Intelligence (PAMI), 2014.<br> CVPR 2013, Oral</br>[<a href="project/cvpr2013/final_collectiveness.pdf" target="_blank">PDF(CVPR)</a>][<a href="project/cvpr2013/pami.pdf" target="_blank">PDF(TPAMI)</a>][<a href="http://mmlab.ie.cuhk.edu.hk/project/collectiveness/">Project Page</a>]
	</div></div>

	<h5 class="pt-2 pb-1">2013 and earlier</h5>
	<div class="publication media">
		<img src="image/cover_cvpr2012.jpg" class="papericon"></td>
           <div class="media-body"><b>Learning Collective Crowd Behaviors with Dynamic Pedestrian-Agents.</b><br>
		Bolei Zhou, Xiaoou Tang and Xiaogang Wang.<br> International Journal of Computer Vision (IJCV), 2014.<br> CVPR 2012, Oral</br>[<a href="project/cvpr2012/zhoucvpr2012.pdf">PDF(CVPR)</a>] [<a href="http://www.ee.cuhk.edu.hk/~xgwang/papers/zhouTWijcv14.pdf" target="_blank">PDF(IJCV)</a>][<a href="http://mmlab.ie.cuhk.edu.hk/project/dynamicagent/" target="_blank">Project Page</a>]
	</div></div>

	<div class="publication media">	
		<a href="http://mmlab.ie.cuhk.edu.hk/project/coherentfiltering/"><img src="image/cover_eccv2012.jpg" class="papericon"></a></td>
           <div class="media-body"><b>Coherent Filtering: Detecting Coherent Motions from Crowd Clutters.</b><br>
		Bolei Zhou, Xiaoou Tang and Xiaogang Wang.<br>ECCV 2012.<br>[<a href="publication/boleieccv2012.pdf">PDF</a>] [<a href="http://mmlab.ie.cuhk.edu.hk/project/coherentfiltering/">Project Page</a>]

	</div></div>
	
	<div class="publication media">
		<a href="http://mmlab.ie.cuhk.edu.hk/project/randomfield/"><img src="image/cover_cvpr2011.jpg" class="papericon"></a><a></a></td>
           <div class="media-body"><b>Random Field Topic Model for Semantic Region Analysis in Crowded Scenes from Tracklets.</b><br>
		Bolei Zhou, Xiaogang Wang and Xiaoou Tang.<br>CVPR 2011<br>[<a href="publication/boleicvpr2011.pdf" target="_blank">PDF</a>][<a href="http://mmlab.ie.cuhk.edu.hk/project/randomfield/" target="_blank">Project Page</a>]
	</div></div>
</div>

	<h2>Students</h2>
		<ul>
			<li>Current: <a href="http://shenyujun.github.io/">Yujun Shen</a>, <a href="http://ceyuan.me/about/">Ceyuan Yang</a>, Zhenghao Peng, Yinghao Xu, Jiankai Sun, <a href="http://anyirao.com/">Anyi Rao</a> (co-advised with Dahua Lin), Hao Sun (co-advised with Dahua Lin), Tiga Leung, <a href="https://zhujiapeng.github.io/">Jiapeng Zhu</a>
			</li>
			<li>Previous: <a href="http://www.cs.princeton.edu/~jw60/">Jimmy Wu</a> (PhD student now at Princeton), <a href="http://tetexiao.com/">Tete Xiao</a> (PhD student now at Berkeley), <a href="https://pbw-berwin.github.io/">Bowen Pan</a> (PhD student now at MIT), <a href="http://www.jasongt.com/">Jinjin Gu</a>(PhD student to be at Sydney U)</li>
		</ul>
	<h2>Teaching</h2>
		<ul>
			<li><a href="https://github.com/zhoubolei/introRL">Intro to Reinforcement Learning (introRL)</a>: Free online lectures</li>
			<li><a href="https://interpretablevision.github.io/">Interpretable Machine Learning for Computer Vision Tutorials</a> at CVPR'20, ICCV'19, CVPR'18</li>
			<li><a href="https://cuhkrlcourse.github.io/">IERG6130 Reinforcement Learning</a> at CUHK, 2nd Term 2018-2019, 2nd Term 2019-2020</li>
			<li><a href="https://course.ie.cuhk.edu.hk/~ierg3050/">IERG3050 Simulation and Statistical Analysis</a> at CUHK, 1st Term 2019-2020</li>
		</ul>
	
    <h2>Honors</h2>
	<div>
        <ul>
        	<li><a href="https://www.cuhk.edu.hk/english/features/zhou-bolei.html">University's Feature Story: Into the Black Box, April 2020</a></li>
        	<li><a href="https://www.innovatorsunder35.com/regions/asiapacific/">MIT Tech Review's Innovators Under 35 in Asia Pacific 2019</a></li>
            <li><a href="https://research.fb.com/fellows/zhou-bolei/">Facebook Fellowship Award 2016-2018</a></li>
            <li>BRC & LING Fellowship Award 2017</li>
            <li>MIT Ho-Ching and Han-Ching Fund Award 2013</li>
            <li>MIT Greater China Computer Science Fellowship 2013</li>
            <li><a href="http://www.ie.cuhk.edu.hk/lnews/13-03-01.shtml">CUHK  Outstanding Thesis Award 2012</a></li>
            <li><a href="http://research.microsoft.com/en-us/collaboration/global/asia-pacific/talent/fellows.aspx#2011" target="_blank">Microsoft Research Asia Fellowship 2011</a></li>
        </ul>    
	</div>


<!--
<div class="text_container">
    <h2>Talks</h2>
	<div>
    <ul>
        <li><a href="ppt/presentation_ICML_workshop.pdf">Interpreting Deep Visual Representations</a> at <a href="http://icmlviz.github.io/">Workshop on Visualization for Deep Learning</a>, ICML'17, Sydney.</li>
        <li><a href="ppt/presentation_CVPR17_oraltalk.pdf">Network Dissection: Quantifying the Interpretability of Deep Visual Representations</a>, CVPR'17, Hawaii.</li>
        <li><a href="http://deeplearning.csail.mit.edu/">Tutorial on the Deep Learning for Objects and Scenes</a>, CVPR'17, Hawaii.</li>
        <li><a href="ppt/understandCNN_tufts.pdf">Understand and Leverage the Internal Representations of CNNs</a> at Tufts, Cornell Tech, Harvard. </li>
        <li><a href="publication/scene_challenges2016.pdf">Challenges in Deep Sceen Understanding</a> at ECCV'16 ILSVRC and COCO joint workshop, Oct. 2016, Amsterdam.</li> 
        <li><a href="http://places.csail.mit.edu/slide_iclr2015.pdf">Object Detectors Emerge in Deep Scene CNNs</a> at ICLR'15, May 2015, San Diego.</li>
        <li><a href="">Learning Deep Features for Scene Recognition</a> at NIPS'14, Dec. 2014, Montreal.</li>
        <li><a href="http://mmlab.ie.cuhk.edu.hk/projects/collectiveness/presentation_cvpr2013.pdf">Measuring Crowd Collectiveness</a> at CVPR'13, June 2013, Portland.</li>
        <li><a href="http://mmlab.ie.cuhk.edu.hk/projects/dynamicagent/presentation_ppt.pdf">Understanding Crowd Behaviors</a> at CVPR'12, June 2012, Rhode Island.</li>
    </ul>
	</div>
</div>

<div class="text_container">
    <h2>Media coverage</h2>
	<div>
    <ul>
        <li><a href="https://venturebeat.com/2018/09/14/mit-csail-designs-ai-that-can-track-objects-over-time/">VentureBeat</a>: MIT CSAIL designs AI that can track objects over time.</li>
        <li><a href="http://news.mit.edu/2018/machine-learning-video-activity-recognition-0914">MIT News</a>: Helping computers fill in the gaps between video frames.</li>
        <li><a href="https://qz.com/1022156/mit-researchers-can-now-track-artificial-intelligences-decisions-back-to-single-neurons/">Quartz</a>: Track AI decisions back to single neurons.</li>
        <li><a href="http://news.mit.edu/2017/inner-workings-neural-networks-visual-data-0630">MIT News</a>: Peering into neural networks.</li>
        <li><a href="https://techcrunch.com/2017/06/30/mit-csail-research-offers-a-fully-automated-way-to-peer-inside-neural-nets/">TechCrunch</a>: A fully automated way to peer inside neural networks.</li>
        <li><a href="https://www.csail.mit.edu/csail_computer_vision_team_leads_scene_parsing_challenge%20">MIT CSAIL News</a>: Scene parsing and scene classification challenges.</li>
        <li><a href="http://techcrunch.com/2015/05/08/ai-project-designed-to-recognize-scenes-surprises-by-identifying-objects-too/" target="_blank">TechCrunch</a> and <a href="http://newsoffice.mit.edu/2015/visual-scenes-object-recognition-0508" target="_blank">MIT News</a>: Object detectors emerge in CNNs.</li>
    </ul>
	</div>
</div>


<div class="text_container"> 
    <h2>Datasets & Benchmarks</h2>
	<div>	
    <ul>
        <li><a href="http://moments.csail.mit.edu">Moments in Time</a>: 1-million video dataset for video scene understanding.</li>
        <li><a href="http://placeschallenge.csail.mit.edu">Places Challenge 2017</a>: instance segmentation, scene parsing, and semantic boundary detection</li>
        <li><a href="http://places2.csail.mit.edu">Places Database</a>: 10 million image database for scene recognition</li>
        <li><a href="https://github.com/CSAILVision/miniplaces">Mini-Places</a>: An educational tool for deep learning in computer vision </li>
        <li><a href="http://sceneparsing.csail.mit.edu/">MIT Scene Parsing Benchmark</a>: full scene semantic segmentation dataset</li>
        <li><a href="http://groups.csail.mit.edu/vision/datasets/ADE20K/">ADE20K dataset</a>: Pixel-wise annotated dataset for semantic scene understanding </li>
    </ul>
	</div>
</div>


<div class="text_container">
	<h2>Open-source softwares</h2>
	<div>
	<ul>
        <li><a href="https://github.com/CSAILVision/semantic-segmentation-pytorch">Semantic Segmentation in PyTorch</a>: an efficient implementation of scene parsing networks trained on ADE20K in PyTorch</a>.
        <li><a href="https://github.com/CSAILVision/NetDissect">Network Dissection</a>: Network visualization and annotation toolkit</a>.
        <li><a href="https://github.com/metalbubble/unitvisseg">CNN Visualizer</a>: Neuron Visualization and Segmentation toolkit for deep CNNs</a>.
        <li><a href="https://github.com/metalbubble/places365">Places365-CNNs</a>: scene recognition networks on Places365 with <a href="https://github.com/metalbubble/places365/tree/master/docker">docker container</a>.</li>
        <li><a href="https://github.com/metalbubble/VQAbaseline/">iBOWIMG</a>: visual question answering baseline code in Torch.</li>  
        <li><a href="https://github.com/metalbubble/CAM">CAM</a>: algorithm package for generating class-specific saliency map for CNN.</li>
	    <li><a href="https://github.com/metalbubble/GoSpark">GoSpark</a>: implementation of Spark, an in-memory distributed computation framework, in Golang [<a href="publication/report_spark.pdf">Report</a>].</li>
        <li><a href="https://github.com/metalbubble/GKLT">gKLT tracker</a>: algorithm package for extracting trajectories from videos with KLT features.</li>
        <li><a href="https://github.com/metalbubble/collectiveness">Collectiveness descriptor</a>: a metric for crowd system order and the simulation of Self-Driven Particles. </li>
        <li><a href="https://github.com/metalbubble/CohFilter">Coherent filtering</a>: algorithm package for detecting coherent motions in time-series data.</li>
        <li><a href="https://github.com/metalbubble/RF_topic">Random field topic model</a>: C++ implementation of MRF on LDA with Gibbs sampling inference.</li>
    </ul>    
</div>
-->

<!--
<div class="text_container">
    <h2>Professional activities</h2>
	<div>
    <ul>
	<li>Co-organizer of the <a href="http://networkinterpretability.org/">AAAI'19 Workshop on Network Interpretability for Deep Learning</a>.</li>
        <li>Organizer of the <a href="https://interpretablevision.github.io/">Tutorial on Interpretable Machine Learning for Computer Vision</a> at CVPR'18.</li>
        <li>Panelist for the <a href="http://interpretable.ml/">NIPS'17 Interpretable Machine Learning Symposium</a>.</li>
        <li>Co-Organizer of the <a href="https://places-coco2017.github.io/">Joint COCO and Places Recognition Challenge Workshop</a> at ICCV'17.</li>
        <li>Organizer of the <a href="http://placeschallenge.csail.mit.edu/">Places Challenge 2017</a> at ICCV'17.</li>
        <li>Organizer of the <a href="http://deeplearning.csail.mit.edu/">Tutorial on Deep Learning for Objects and Scenes</a> at CVPR'17.</li>
        <li>Organizer of the <a href="http://sunw.csail.mit.edu/">5th Scene Understanding Workshop</a> at CVPR'17</li>
        <li>Organizer of the <a href="http://places2.csail.mit.edu/results2016.html">Places365 Challenge 2016</a> and <a href="http://sceneparsing.csail.mit.edu/index_challenge.html">Scene Parsing Challenge 2016</a> at ECCV'16.</li>
        <li>Co-organizer of <a href='http://image-net.org/challenges/LSVRC/2016/index'>ILSVRC'16 challenge workshop</a> at ECCV'16
        <li>Organizer of the <a href='http://places2.csail.mit.edu/results2015.html'>Places Challenge 2015</a> in ICCV'15.</li>
        <li>Conference reviewer for ICCV'17, BMVC'17, CVPR'17, ACCV'16, ECCV'16, CVPR'16, ICCV'15, CVPR'15, ECCV'14, ACCV'14.</li>
	    <li>Journal reviewer for TPAMI, IJCV, The visual computer, Computer Vision and Image Understanding, IEEE Trans on NNLS, IEEE Trans on IP, IEEE Trans on SMC, IEEE Trans on CSVT, PLOS ONE, Pattern Recognition.</li>
        <li>Teaching Assistant for MIT course <a href="http://6.869.csail.mit.edu/fa15" target="_blank">Advances in Computer Vision</a>. In the course a <a href="http://6.869.csail.mit.edu/fa15/project.html" target="_blank">Mini-Places Scene Classification Challenge</a> is hosted for educational purpose.</li>
        <li>Chair of the <a href="https://sites.google.com/view/visionseminar">MIT Vision Seminar</a>.</li>
        <li>Internships at Facebook AI Research, eBay Research Labs, Microsoft Research Asia, and Barclays Capital.</li>
    </ul>        
	</div>
</div>

    <h2>Collaborators</h2>
    <ul>
        <li>I am fortunate to work with these great people: <a href="http://cvcl.mit.edu/Aude.htm">Aude Oliva</a>(MIT), <a href="http://vision.princeton.edu/people/xj/">Jianxiong Xiao</a>(Princeton), <a href="http://www.cvc.uab.es/~agata/">Agata Lapedriza</a>(UOC), <a href="http://dusp.mit.edu/faculty/jinhua-zhao">Jinhua Zhao</a>(MIT), <a href="http://www.ee.cuhk.edu.hk/~xgwang/">Xiaogang Wang</a>(CUHK), <a href="http://www.ie.cuhk.edu.hk/people/xotang.shtml">Xiaoou Tang</a>(CUHK), <a href="http://ins.sjtu.edu.cn/faculty/zhanghepeng">Hepeng Zhang</a>(SJTU), <a href="http://bcmi.sjtu.edu.cn/~zhangliqing/">Liqing Zhang</a>(SJTU), <a href="http://www.houxiaodi.com/">Xiaodi Hou</a>(Caltech), <a href="http://liuliu.us/">Liu Liu</a>(MIT), <a href="http://people.csail.mit.edu/khosla/">Aditya Khosla (MIT)</a>, Robinson Piramuthu(eBay Research Labs), Vignesh Jagadeesh(eBay Research Labs), Yuandong Tian(FB), Rob Fergus(NYU&FB), Arthur Szlam(FB), Sainbayar
        Sukhbaatar(NYU), Zi Wang (MIT), Stefanie Jegelka (MIT), Hang Zhao (MIT), Xavier Puig (MIT), Sanja Fidler (UToronto), Larry Zitnick(FB).</li>
    </ul>
<div class="text_container">
    <h2>Personal interests</h2>
    <ul>
    <li>blogs:<a href="http://urbancomputation.wordpress.com/" target="_parent">Urban Computation</a>,<a href="https://crowdbehaviordotorg.wordpress.com/" target="_parent">Crowd Behavior &amp; Psychology</a></li>
    <li><a href="book.html">books</a>, <a href="image/beacon_hill.jpg">rock climbing (5.11C,V6)</a>, <a href="bolei_juggle.mp4">juggling</a> (recently), <a href="image/bass.jpg">bass player</a> (former <a href="image/i3.jpg">lead guitarist</a>)</a> </li>
    </ul>
</div>

   --> 

</div>
</div>
</body></html>
