
 <!DOCTYPE html>

<html><head>
<title>Rui Yan</title>

<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">

<script src="https://code.jquery.com/jquery-3.1.1.slim.min.js" integrity="sha384-A7FZj7v+d/sdmMqp/nOQwliLvUsJfDHW+k9Omg/a/EheAdgtzNs3hpfag6Ed950n" crossorigin="anonymous"></script>

<style type="text/css">
 @import url("http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,300italic,600,600italic");


	body
	{
	font-family:"Roboto",Helvetica,Arial,sans-serif;font-size:16px;line-height:1.5;font-weight:300;
    	background-color : #CDCDCD;
	}
    	.content
	{
    		width : 900px;
    		padding : 25px 30px;
    		margin : 25px auto;
    		background-color : #fff;
    		box-shadow: 0px 0px 10px #999;
    		border-radius: 15px; 
	}	
	table
	{
		padding: 5px;
	}
	
	table.pub_table,td.pub_td1,td.pub_td2
	{
		padding: 8px;
		width: 850px;
        border-collapse: separate;
        border-spacing: 15px;
        margin-top: -5px;
	}

	td.pub_td1
	{
		width:50px;
	}
    td.pub_td1 img
    {
        height:120px;
        width: 160px;
    }
	
	div#container
	{
		margin-left: auto;
		margin-right: auto;
		width: 820px;
		text-align: left;
		position: relative;
		background-color: #FFF;
	}
	div#DocInfo
	{
		color: #1367a7;
		height: 158px;
	}
	h4,h3,h2,h1
	{
		color: #3B3B3B;
	}
	h2
	{
		font-size:130%;
	}
	p
	{
		color: #5B5B5B;
		margin-bottom: 50px;
	}
	p.caption
	{
		color: #9B9B9B;
		text-align: left;
		width: 600px;
	}
	p.caption2
	{
		color: #9B9B9B;
		text-align: left;
		width: 800px;
	}
	#header_img
	{
		position: absolute;
		top: 0px; right: 0px;
    }
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}

    #mit_logo {
        position: absolute;
        left: 646px;
        top: 14px;
        width: 200px;
        height: 20px;
    }
   
    table.pub_table tr {
        outline: thin dotted #666666;
    }
    .papericon {
        border-radius: 8px; 
        -moz-box-shadow: 3px 3px 6px #888;
        -webkit-box-shadow: 3px 3px 6px #888;
        box-shadow: 3px 3px 6px #888;
        width: 180px;
	margin-top:5px;
	margin-left:5px;
	margin-bottom:5px;
    }

     .papericon_blank {

        width: 160px;
	margin-top:5px;
	margin-left:5px;
	margin-bottom:5px;
    }

    .media {
	outline: thin dotted #666666;
 	margin-bottom: 15px;	
	margin-left:10px;
    }
    .media-body {
	margin-top:5px;
	padding-left:20px;
    }

.papers-selected h5, .papers-selected h4 { display : none; }
.papers-selected .publication { display : none; }
.paperhi-only { display : none; }
.papers-selected .paperhi { display : flex; }
.papers-selected .paperlo { display : none; }

.hidden>div {
	display:none;
}

.visible>div {
	display:block;
}
</style>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-23931362-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-23931362-2');
</script>

<script type="text/javascript">
    var myPix = new Array("img/YanR.jpeg")
    function choosePic() {
        var randomNum = Math.floor(Math.random() * myPix.length);
        document.getElementById("myPicture").src = myPix[randomNum];
    };
</script>

<script>
$(document).ready(function() {
  $('.paperlo button').click(function() {
     $('.papers-container').addClass('papers-selected');
  });
  $('.paperhi button').click(function() {
     $('.papers-container').removeClass('papers-selected');
  });


	$('.text_container').addClass("hidden");

	$('.text_container').click(function() {
		var $this = $(this);

		if ($this.hasClass("hidden")) {
			$(this).removeClass("hidden").addClass("visible");
			$(this).removeClass("papericon");
		} else {
			$(this).removeClass("visible").addClass("hidden");
		}
	});


});
</script>

</head>


<body>
<div class="content">
	<div id="container">

	<table>
	<tbody><tr>
	<td><img id="myPicture" src="xxx" style="float:left; padding-right:20px" height="200px"></td>
	<script>choosePic();</script>
	<td>
	<div id="DocInfo">
        <h1>Rui Yan</h1>
		Assistant Researcher<br>
		Department of Computer Science and Technology, Nanjing University<br>
		Office: Room 819, Building of Computer Science and Technology, Xianlin Campus of Nanjing University, Nanjing 210023, China<br>
		Email: ruiyan _at_ nju.edu.cn<br>
		<!-- PhD student<br>
		School of Computer Science and Engineering, Nanjing University of Science and Technology<br>
		Office: Room 2003, CSE Building<br>
        Email: ruiyan _at_ njust.edu.cn<br> -->
        <a href="">CV</a> &bull; <a href="https://scholar.google.com.hk/citations?user=PWy5LfMAAAAJ&hl=zh-CN&oi=ao">Google Scholar</a> &bull; <a href="https://github.com/ruiyan1995">Github</a> &bull; <a href="">Twitter</a><br>
	</div><br>
    <!--
    <div id="mit_logo">
        <a href="http://www.mit.edu"><img src="image/mit.gif" height="170px" class="papericon" /></a>
    </div>
    -->
	</td>
	</tr>
	</tbody></table>
	<br>


	<h2>Intro</h2>
        <p style="text-align:justify";>
		Currently, I am an Assistant Researcher of Department of Computer Science and Technology at Nanjing University and working with <a href="https://scholar.google.com/citations?user=W-FGd_UAAAAJ&hl=en">Tieniu Tan</a>.
		I obtained Ph.D. degree at <a href="https://imag-njust.net">Intelligent Media Analysis Group (IMAG), Nanjing University of Science and Technology</a>, under the supervision of Prof. <a href="https://imag-njust.net/jinhui-tang">Jinhui Tang</a>. 
		During Jan. 2022 - Aug. 2022, I worked as a Research Intern (Part-time) at ByteDance with <a href="https://songbai.site/">Song Bai</a>. 
		During Sep. 2021 - Dec. 2021, I worked as a Research Intern (Part-time) at Tencent with <a href="https://geyixiao.com/">Yixiao Ge</a>. 
		During Dec. 2018 - Dec. 2019, I worked as a Research Intern at <a href="http://www.noahlab.com.hk/">HUAWEI NOAH'S ARK LAB</a> with <a href="http://lingxixie.com/Home.html">Lingxi Xie</a> and Prof. <a href="https://scholar.google.com/citations?user=61b6eYkAAAAJ&hl=en">Qi Tian</a> (IEEE Fellow). 
		I am working closely with <a href="http://www.columbia.edu/~zs2262/">Mike Shou</a> and <a href="https://imag-njust.net/xiangboshu">Xiangbo Shu</a>. 
		My research mainly focus on Complex Human Behavior Understanding and Video-Language Understanding.
    	</p>
	
    <h2>News</h2>
    <ul>
		<li>2023.6: Four paper is accepted by ACM MM 2023.</li>
		<li>2023.6: One paper is accepted by ICCV 2023.</li>
		<li>2023.5: One paper is accepted by TCSVT.</li>
		<li>2023.3: One paper is accepted by TPAMI.</li>
		<li>2023.2: One paper is accepted by CVPR 2023.</li>
		<li>2022.11: One paper is accepted by AAAI 2023.</li>
		<li>2022.09: One paper is accepted by NeurIPS 2022.</li>
		<li>2022.07: One paper is accepted by ACM MM 2022.</li>
		<li>2022.06: Our team achieves the First Place Award in Object State Change Classification Track, the Second Place Award in Natural Language Queries for Episodic Memory Track, and the Third Place Award in PNR Temporal Localization Track of EGO4D Challenge (CVPR 2022).</li>
		<li>2022.06: Our team achieves the First Place Award in Multi-Instance Action Retrieval Track of EPIC-Kitchens Dataset Challenges (CVPR 2022).</li>
		<li>2022.03: Two papers are accepted by CVPR 2022.</li>
		<li>2022.01: One paper is accepted by TCSVT.</li>
		<li>2021.12: I give a talk about Video-Language Pre-training at PCG, Tencent.</li>
		<li>2021.08: I will work with Prof. <a href="http://www.columbia.edu/~zs2262/">Mike Shou</a> at <a href="https://sites.google.com/view/showlab/">Show Lab</a>, National University of Singapore.</li>
		<li>2020.05.19: One paper is accepted by TNNLS.</1i>
		<li>2020.12.08: One paper is accepted by ACM MM Asia 2020.</1i>
		<li>2020.10.20: One paper is accepted by T-PAMI.</1i>
    	<!-- <li>2020.08.17: One paper is accepted by China MM 2020 and recommended to Multimedia System.</li> -->
    	<!-- <li>2020.08.05: I achieve the Chinese Government Scholarship.</li> -->
		<!-- <li>2020.08.05: I achieve the Chinese Government Scholarship and will work with Prof. Wanli Ouyang at The University of Sydney in the next two years.</li> -->
    	<li>2020.07.03: One paper is accepted by ECCV 2020.</li>
    	<li>2020.05: Selected as the Outstanding PhD of NJUST.</li>
    	<li>2019.05: I give a talk about GAR at the Noah's Ark Lab, Huawei Inc.</li>
    </ul>


<div class="papers-container papers-selected"> 
	<h5 class="paperlo">All Publications<button type="button" class="ml-3 btn btn-light"> Show selected</button></h5>
	<h5 class="paperhi paperhi-only">Selected Publications<button type="button" class="ml-3 btn btn-light"> Show all</button></h5>
	<h5 class="pt-2 pb-1">2020</h5>	


	<div class="publication media paperhi">
		<img src="img/Interactive_Fusion.png" height="100" width="200" class="papericon">
		<div class="media-body"><b>Progressive Instance-aware Feature Learning for Compositional Action Recognition</b><br>
			<b>Rui Yan</b>, Lingxi Xie, Xiangbo Shu, Liyan Zhang, and Jinhui Tang<br>
			TPAMI, 2023<br>
			[<a href="https://arxiv.org/pdf/2012.05689">PDF</a>][<a href="https://github.com/ruiyan1995/Interactive_Fusion_for_CAR">Code</a>]
	 	</div>
	</div>
	
	<div class="publication media paperhi">
		<img src="img/ALLINONE.jpeg" height="100" width="200" class="papericon">
		<div class="media-body"><b>All in One: Exploring Unified Video-language Pre-training</b><br>
			Jinpeng Wang, Yixiao Ge, <b>Rui Yan</b>, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, Xiaohu Qie, Mike Zheng Shou<br>
			CVPR 2023<br>
			[<a href="https://arxiv.org/abs/2203.07303">PDF</a>][<a href="https://github.com/showlab/all-in-one">Code</a>]
	 	</div>
	</div>

	<!-- <div class="publication media paperhi">
		<img src="img/DemoVLP.png" height="100" width="200" class="papericon">
		<div class="media-body"><b>Revitalize Region Feature for Democratizing Video-Language Pre-training</b><br>
			Guanyu Cai, Yixiao Ge, Jinpeng Wang, <b>Rui Yan</b>, Xudong Lin, Ying Shan, Lianghua He, Jianping Wu, Xiaohu Qie, Mike Zheng Shou<br>
			Under Review<br>
			[<a href="https://arxiv.org/abs/2203.07720">PDF</a>][<a href="https://github.com/CuthbertCai/DemoVLP">Code</a>]
	 	</div>
	</div> -->



	
	<div class="publication media paperhi">
		<img src="img/RegionLearner.jpg" height="100" width="200" class="papericon">
		<div class="media-body"><b>Video-Text Pre-training with Learned Regions for Retrieval</b><br>
			<b>Rui Yan</b>, Mike Zheng Shou, Yixiao Ge, Alex Jinpeng Wang, Xudong Lin, Guanyu Cai, and Jinhui Tang<br>
			AAAI 2023<br>
			[<a href="https://arxiv.org/pdf/2112.01194">PDF</a>][<a href="https://github.com/ruiyan1995/Region_Learner">Code</a>]
	 	</div>
	</div>

	<div class="publication media paperhi">
		<img src="img/EGO_VLP.png" height="100" width="200" class="papericon">
		<div class="media-body"><b>Egocentric Video-Language Pretraining</b><br>
			Kevin Qinghong Lin, Alex Jinpeng Wang, Mattia Soldan, Michael Wray, <b>Rui Yan</b>, Eric Zhongcong Xu, Difei Gao, Rongcheng Tu, Wenzhe Zhao, Weijie Kong, Chengfei Cai, Hongfa Wang, Dima Damen, Bernard Ghanem, Wei Liu, Mike Zheng Shou<br>
			NeurIPS 2022 (Spotlight)<br>
			[<a href="https://arxiv.org/pdf/2206.01670">PDF</a>][<a href="https://github.com/showlab/EgoVLP">Code</a>]
	 	</div>
	</div>

	<div class="publication media paperhi">
		<img src="img/LLTM.png" height="100" width="200" class="papericon">
		<div class="media-body"><b>Look Less Think More: Rethinking Compositional Action Recognition</b><br>
			<b>Rui Yan</b>, Peng Huang, Xiangbo Shu, Junhao Zhang, Yonghua Pan, Jinhui Tang<br>
			ACM MM 2022<br>
			[<a href="https://dl.acm.org/doi/abs/10.1145/3503161.3547862">PDF</a>][<a href="https://drive.google.com/file/d/1UawPdVymHlfl2C9sAtUWmTri529jM7oV/view?usp=share_link">Split</a>]
	 	</div>
	</div>


	<div class="publication media paperhi">
		<img src="img/Dual-AI.png" height="100" width="200" class="papericon">
		<div class="media-body"><b>Dual-AI: Dual-path Actor Interaction Learning for Group Activity Recognition</b><br>
			Mingfei Han, David Junhao Zhang, Yali Wang, <b>Rui Yan</b>, Lina Yao, Xiaojun Chang, and Yu Qiao<br>
			CVPR 2022 (Oral)<br>
			[<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Han_Dual-AI_Dual-Path_Actor_Interaction_Learning_for_Group_Activity_Recognition_CVPR_2022_paper.pdf">PDF</a>][<a href="xx">Code</a>]
	 	</div>
	</div>

	<div class="publication media paperhi">
		<img src="img/none.jpeg" height="100" width="200" class="papericon">
		<div class="media-body"><b>Magi-Net: Meta Negative Network for Early Activity Prediction</b><br>
			Wenqian Wang, Faliang Chang, David Junhao Zhang, <b>Rui Yan</b>, Chunsheng Liu, Bin Wang, Mike Zheng Shou<br>
			IEEE Transactions on Image Processing 2023<br>
			[<a href="xx">PDF</a>][<a href="xx">Code</a>]
	 	</div>
	</div>



	<div class="publication media paperhi">
		<img src="img/OA-T.png" height="100" width="200" class="papericon">
		<div class="media-body"><b>Object-aware Video-language Pre-training for Retrieval</b><br>
			Alex Jinpeng Wang, Yixiao Ge, Guanyu Cai, <b>Rui Yan</b>, Xudong Lin, Ying Shan, Xiaohu Qie, and Mike Zheng Shou<br>
			CVPR 2022<br>
			[<a href="https://arxiv.org/pdf/2112.00656">PDF</a>][<a href="https://github.com/FingerRec/OA-Transformer">Code</a>]
	 	</div>
	</div>



	<div class="publication media paperhi">
		<img src="img/ESE-FN.png" height="100" width="200" class="papericon">
		<div class="media-body"><b>Expansion-Squeeze-Excitation Fusion Network for Elderly Activity Recognition</b><br>
			Xiangbo Shu, Jiawen Yang, <b>Rui Yan (Corresponding Author)</b>, and Yan Song<br>
			TCSVT 2022<br>
			[<a href="https://arxiv.org/pdf/2112.10992">PDF</a>]
	 	</div>
	</div>





	<div class="publication media paperhi">
		<img src="img/HiGCIN.png" height="100" width="200" class="papericon">
		<div class="media-body"><b>HiGCIN: Hierarchical Graph-based Cross Inference Network for Group Activity Recognition</b><br>
			<b>Rui Yan</b>, Lingxi Xie, Jinhui Tang, Xiangbo Shu, and Qi Tian<br>
			TPAMI, 2020<br>
			[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9241410">PDF</a>][<a href="https://github.com/ruiyan1995/HiGCIN">Code</a>]
	 	</div>
	</div>

	<div class="publication media paperhi">
           <img src="img/SAM.png" height="100" width="200" class="papericon">
           <div class="media-body"><b>Social Adaptive Module for Weakly-supervised Group Activity Recognition</b><br>
           		<b>Rui Yan</b>, Lingxi Xie, Jinhui Tang, Xiangbo Shu, and Qi Tian<br>
           		ECCV 2020 <br>
           		[<a href="https://arxiv.org/pdf/2007.09470">PDF</a>][<a href="SAM.html">Project</a>][<a href="https://github.com/ruiyan1995/Weakly-supervised-Group-Activiy-Recognition">Code</a>]
			</div>
	</div>


	<!-- <div class="publication media paperhi">
           <img src="img/SRM.png" height="100" width="200" class="papericon">
           <div class="media-body"><b>Storyboard Region Relationship Model for Group Activity Recognition</b><br>
           		Boning Li, Xiangbo Shu, and <b>Rui Yan</b><br>
           		ACM MM Asia 2020<br>
           		[<a href="xxx">PDF</a>]
			</div>
	</div> -->


	<div class="publication media paperhi">
           <img src="img/CCGL.png" height="100" width="200" class="papericon">
           <div class="media-body"><b>Coherence Constrained Graph LSTM for Group Activity Recognition</b><br>
           		Jinhui Tang, Xiangbo Shu, <b>Rui Yan</b>, and Liyan Zhang<br>
           		TPAMI, 2019<br>
           		[<a href="xxx">PDF</a>]
			</div>
	</div>

	<div class="publication media paperhi">
           <img src="img/PC-TDM.jpg" height="100" width="200" class="papericon">
           <div class="media-body"><b>Participation-Contributed Temporal Dynamic Model for Group Activity Recognition</b><br>
           		<b>Rui Yan</b>, Jinhui Tang, Xiangbo Shu, Zechao Li and Qi Tian<br>
           		ACM MM 2018 (Oral) ~8.5% (Journal version is accepted by TNNLS)<br>
           		[<a href="https://www.researchgate.net/profile/Rui_Yan31/publication/328372578_Participation-Contributed_Temporal_Dynamic_Model_for_Group_Activity_Recognition/links/5bed27684585150b2bb79e69/Participation-Contributed-Temporal-Dynamic-Model-for-Group-Activity-Recognition.pdf">PDF</a>][<a href="https://github.com/ruiyan1995/Group-Activity-Recognition">Code</a>][<a href="https://github.com/ruiyan1995/ruiyan1995.github.io/raw/master/mm2018_korea.pptx">Slides</a>]
			</div>
	</div>

	

	<!-- <div class="publication media paperhi">
           <img src="img/Skip-Attention.jpg" height="100" width="200" class="papericon">
           <div class="media-body"><b>Skip-Attention Encoder-Decoder Framework for Human Motion Prediction</b><br>
           		Ruipeng Zhang, Xiangbo Shu, <b>Rui Yan</b>, Jiachao Zhang and Yan Song<br>
           		China MM 2020 (recommended to Multimedia Systems)<br>
           		[<a href="">PDF</a>]
			</div>
	</div> -->

	<!-- <div class="publication media paperhi">
           <img src="img/none.jpeg" height="100" width="200" class="papericon">
           <div class="media-body"><b>A Feature Selection Method for Projection Twin Support Vector Machine</b><br>
           		<b>Rui Yan</b>, Qiaolin Ye, Liyan Zhang, Ning Ye and Xiangbo Shu<br>
           		Neural Processing Letters, 2016<br>
           		[<a href="https://www.researchgate.net/profile/Rui_Yan31/publication/317769500_A_Feature_Selection_Method_for_Projection_Twin_Support_Vector_Machine/links/59b1370d458515a5b4890247/A-Feature-Selection-Method-for-Projection-Twin-Support-Vector-Machine.pdf">PDF</a>]
			</div>
	</div> -->



</div>
    <h2>Awards & Honors</h2>
	<div>
        <ul>
			<li>南京大学-毓秀青年学者 2023</li>
			<li>江苏省卓越博士后 2023</li>
        	<li>The Outstanding PhD of Nanjing University of Science and Technology 2020</li>
        	<li>ACM Multimedia Student Travel Grant 2018</li>
        	<li>The First Prize Scholarship of Nanjing University of Science and Technology 2017, 2018</li>
            <li>Excellent Undergraduate Thesis of Jiangsu Province 2018</li>
            <li>Top Ten Outstanding Youth of Nanjing Forestry University 2017</li>
            <li>The Second prize of National University Students Computer Design Competition 2016</li>
            <li>The National Encouragement Scholarship 2014, 2015, 2016</li>
            <li>Merit Student of Nanjing Forestry University 2014, 2015, 2016</li>
        </ul>    
	</div>

	<h2>Grants</h2>
	<div>
        <ul>
			<li>中国博士后科学基金第73批面上资助(80K), 2023-2025</li>
			<li>中国博士后科学基金特别资助(180K), 2023-2025</li>
			<li>国家自然科学基金青年科学基金项目(100K), 2024.1-2024.12</li>
			<li>江苏省卓越博士后资助(300K), 2023-2025</li>
			<li>南京大学-毓秀青年学者, 2023-2026</li>
			<li>集成公关大平台-揭榜挂帅(200K), 2023-2025</li>
        </ul>    
	</div>


	<h2>Professional Services</h2>
	<div>
		<ul>
			<li>
				TPC for IEEE MIPR 2020 and BigMM 2021.
			</li>
			<li>
				Reviewer for CVPR/ICCV/ECCV/AAAI/IJCAI, TIP/TNNLS/TCSVT/PR/Neurocomputing/Information Sciences.
			</li>
			<li>
				Associate Editor for Computer Systems Science and Engineering
			</li>
			<!-- <li> -->
				<!-- Reviewer for Information Sciences, TIP, TNNLS, TCSVT, PR, Neurocomputing. -->
				<!-- Multimedia Systems, The Visual Computer, KSII Transactions on Internet and Information Systems. -->
			<!-- </li> -->
			<!-- <li> -->
				<!-- Reviewer for CVPR, ICCV, ECCV, AAAI. -->
				<!-- IJCNN 2021/2022, PCM 2018, ICMBD 2018. -->
			<!-- </li> -->
		</ul>
	</div>

	<!-- <h2>My Friends</h2>
	<a href="https://fingerrec.github.io/">Alex Wang</a>,
	<a href="https://junhaozhang98.github.io/">David Zhang</a>,
	<a href="xxx">Guanyu Cai</a>,
	<a href="https://dongzhang89.github.io/">Dong Zhang</a>,
	<a href="https://cser-tang-hao.github.io/">Hao Tang</a>,
	<a href="xxx">Peng Huang</a>.
	<a href="https://scholar.google.com/citations?user=SI37kGYAAAAJ&hl=zh-CN">Shuqing Bian</a>. -->

	
	

	<!-- <h2>Hobbies</h2> -->

</div>



</div>
<a href='http://bzhou.ie.cuhk.edu.hk/'>Source stolen from here</a>
</body></html>
