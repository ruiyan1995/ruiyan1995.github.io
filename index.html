
 <!DOCTYPE html>

<html><head>
<title>Rui Yan</title>

<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">

<script src="https://code.jquery.com/jquery-3.1.1.slim.min.js" integrity="sha384-A7FZj7v+d/sdmMqp/nOQwliLvUsJfDHW+k9Omg/a/EheAdgtzNs3hpfag6Ed950n" crossorigin="anonymous"></script>

<style type="text/css">
 @import url("http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,300italic,600,600italic");


	body
	{
	font-family:"Roboto",Helvetica,Arial,sans-serif;font-size:16px;line-height:1.5;font-weight:300;
    	background-color : #CDCDCD;
	}
    	.content
	{
    		width : 900px;
    		padding : 25px 30px;
    		margin : 25px auto;
    		background-color : #fff;
    		box-shadow: 0px 0px 10px #999;
    		border-radius: 15px; 
	}	
	table
	{
		padding: 5px;
	}
	
	table.pub_table,td.pub_td1,td.pub_td2
	{
		padding: 8px;
		width: 850px;
        border-collapse: separate;
        border-spacing: 15px;
        margin-top: -5px;
	}

	td.pub_td1
	{
		width:50px;
	}
    td.pub_td1 img
    {
        height:120px;
        width: 160px;
    }
	
	div#container
	{
		margin-left: auto;
		margin-right: auto;
		width: 820px;
		text-align: left;
		position: relative;
		background-color: #FFF;
	}
	div#DocInfo
	{
		color: #1367a7;
		height: 158px;
	}
	h4,h3,h2,h1
	{
		color: #3B3B3B;
	}
	h2
	{
		font-size:130%;
	}
	p
	{
		color: #5B5B5B;
		margin-bottom: 50px;
	}
	p.caption
	{
		color: #9B9B9B;
		text-align: left;
		width: 600px;
	}
	p.caption2
	{
		color: #9B9B9B;
		text-align: left;
		width: 800px;
	}
	#header_img
	{
		position: absolute;
		top: 0px; right: 0px;
    }
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}

    #mit_logo {
        position: absolute;
        left: 646px;
        top: 14px;
        width: 200px;
        height: 20px;
    }
   
    table.pub_table tr {
        outline: thin dotted #666666;
    }
    .papericon {
        border-radius: 8px; 
        -moz-box-shadow: 3px 3px 6px #888;
        -webkit-box-shadow: 3px 3px 6px #888;
        box-shadow: 3px 3px 6px #888;
        width: 180px;
	margin-top:5px;
	margin-left:5px;
	margin-bottom:5px;
    }

     .papericon_blank {

        width: 160px;
	margin-top:5px;
	margin-left:5px;
	margin-bottom:5px;
    }

    .media {
	outline: thin dotted #666666;
 	margin-bottom: 15px;	
	margin-left:10px;
    }
    .media-body {
	margin-top:5px;
	padding-left:20px;
    }

.papers-selected h5, .papers-selected h4 { display : none; }
.papers-selected .publication { display : none; }
.paperhi-only { display : none; }
.papers-selected .paperhi { display : flex; }
.papers-selected .paperlo { display : none; }

.hidden>div {
	display:none;
}

.visible>div {
	display:block;
}
</style>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-23931362-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-23931362-2');
</script>

<script type="text/javascript">
    var myPix = new Array("img/YanR.jpeg")
    function choosePic() {
        var randomNum = Math.floor(Math.random() * myPix.length);
        document.getElementById("myPicture").src = myPix[randomNum];
    };
</script>

<script>
$(document).ready(function() {
  $('.paperlo button').click(function() {
     $('.papers-container').addClass('papers-selected');
  });
  $('.paperhi button').click(function() {
     $('.papers-container').removeClass('papers-selected');
  });


	$('.text_container').addClass("hidden");

	$('.text_container').click(function() {
		var $this = $(this);

		if ($this.hasClass("hidden")) {
			$(this).removeClass("hidden").addClass("visible");
			$(this).removeClass("papericon");
		} else {
			$(this).removeClass("visible").addClass("hidden");
		}
	});


});
</script>

</head>


<body>
<div class="content">
	<div id="container">

	<table>
	<tbody><tr>
	<td><img id="myPicture" src="xxx" style="float:left; padding-right:20px" height="200px"></td>
	<script>choosePic();</script>
	<td>
	<div id="DocInfo">
		<h1>Rui Yan</h1>
        PhD student<br>
	School of Computer Science and Engineering, Nanjing University of Science and Technology<br>
		Office: Room 2003, CSE Building<br>
        Email: ruiyan _at_ njust.edu.cn<br>
        <a href="">CV</a> &bull; <a href="https://scholar.google.com.hk/citations?user=PWy5LfMAAAAJ&hl=zh-CN&oi=ao">Google Scholar</a> &bull; <a href="https://github.com/ruiyan1995">Github</a> &bull; <a href="">Twitter</a><br>
	</div><br>
    <!--
    <div id="mit_logo">
        <a href="http://www.mit.edu"><img src="image/mit.gif" height="170px" class="papericon" /></a>
    </div>
    -->
	</td>
	</tr>
	</tbody></table>
	<br>


	<h2>Intro</h2>
        <p style="text-align:justify";>
        I am a Ph.D. student at <a href="https://imag-njust.net">Intelligent Media Analysis Group (IMAG)</a> supervised by Prof. <a href="https://imag-njust.net/jinhui-tang">Jinhui Tang</a>. During Dec. 2018 - Dec. 2019, I worked as a Research Intern at <a href="http://www.noahlab.com.hk/">HUAWEI NOAH'S ARK LAB</a>, supervised by Prof. <a href="https://scholar.google.com/citations?user=61b6eYkAAAAJ&hl=en">Qi Tian</a> (IEEE Fellow). Now, I am working closely with <a href="http://lingxixie.com/Home.html">Lingxi Xie</a> and <a href="https://imag-njust.net/xiangboshu">Xiangbo Shu</a>. My research mainly focuses on visual reasoning and its applications in action understanding. In particular, I am interested in group activity recognition, compositional action recognition, and action localization/detection.
    	</p>
	
    <h2>News</h2>
    <ul>
		<li>2022.01: One paper is accepted by TCSVT.</li>
		<li>2021.12: I give a talk about Video-Language Pre-training at PCG, Tencent.</li>
		<li>2021.08: I will work with Prof. <a href="http://www.columbia.edu/~zs2262/">Mike Shou</a> at <a href="https://sites.google.com/view/showlab/">Show Lab</a>, National University of Singapore.</li>
		<li>2020.05.19: One paper is accepted by TNNLS.</1i>
		<li>2020.12.08: One paper is accepted by ACM MM Asia 2020.</1i>
		<li>2020.10.20: One paper is accepted by T-PAMI.</1i>
    	<!-- <li>2020.08.17: One paper is accepted by China MM 2020 and recommended to Multimedia System.</li> -->
    	<li>2020.08.05: I achieve the Chinese Government Scholarship.</li>
		<!-- <li>2020.08.05: I achieve the Chinese Government Scholarship and will work with Prof. Wanli Ouyang at The University of Sydney in the next two years.</li> -->
    	<li>2020.07.03: One paper is accepted by ECCV 2020.</li>
    	<li>2020.05: Selected as the Outstanding PhD of NJUST.</li>
    	<li>2019.05: I give a talk about GAR at the Noah's Ark Lab, Huawei Inc.</li>
    </ul>


<div class="papers-container papers-selected"> 
	<h5 class="paperlo">All Publications<button type="button" class="ml-3 btn btn-light"> Show selected</button></h5>
	<h5 class="paperhi paperhi-only">Selected Publications<button type="button" class="ml-3 btn btn-light"> Show all</button></h5>
	<h5 class="pt-2 pb-1">2020</h5>	

	<div class="publication media paperhi">
		<img src="img/RegionLearner.jpg" height="100" width="200" class="papericon">
		<div class="media-body"><b>Video-Text Pre-training with Learned Regions</b><br>
			<b>Rui Yan</b>, Mike Zheng Shou, Yixiao Ge, Alex Jinpeng Wang, Xudong Lin, Guanyu Cai, and Jinhui Tang<br>
			Under Review<br>
			[<a href="https://arxiv.org/pdf/2112.01194">PDF</a>][<a href="https://github.com/ruiyan1995/Region_Learner">Code</a>]
	 	</div>
	</div>

	<div class="publication media paperhi">
		<img src="img/OA-T.png" height="100" width="200" class="papericon">
		<div class="media-body"><b>Object-aware Video-language Pre-training for Retrieval</b><br>
			Alex Jinpeng Wang, Yixiao Ge, Guanyu Cai, <b>Rui Yan</b>, Xudong Lin, Ying Shan, Xiaohu Qie, and Mike Zheng Shou<br>
			Under Review<br>
			[<a href="https://arxiv.org/pdf/2112.00656">PDF</a>][<a href="https://github.com/FingerRec/OA-Transformer">Code</a>]
	 	</div>
	</div>

	<div class="publication media paperhi">
		<img src="img/ESE-FN.png" height="100" width="200" class="papericon">
		<div class="media-body"><b>Expansion-Squeeze-Excitation Fusion Network for Elderly Activity Recognition</b><br>
			Xiangbo Shu, Jiawen Yang, <b>Rui Yan</b>, and Yan Song<br>
			TCSVT<br>
			[<a href="https://arxiv.org/pdf/2112.10992">PDF</a>]
	 	</div>
	</div>

	<div class="publication media paperhi">
		<img src="img/Interactive_Fusion.png" height="100" width="200" class="papericon">
		<div class="media-body"><b>Interactive Fusion of Multi-level Features for Compositional Activity Recognition</b><br>
			<b>Rui Yan</b>, Lingxi Xie, Xiangbo Shu, and Jinhui Tang<br>
			Under Review, 2020<br>
			[<a href="https://arxiv.org/pdf/2012.05689">PDF</a>][<a href="https://github.com/ruiyan1995/Interactive_Fusion_for_CAR">Code</a>]
	 	</div>
	</div>



	<div class="publication media paperhi">
		<img src="img/HiGCIN.png" height="100" width="200" class="papericon">
		<div class="media-body"><b>HiGCIN: Hierarchical Graph-based Cross Inference Network for Group Activity Recognition</b><br>
			<b>Rui Yan</b>, Lingxi Xie, Jinhui Tang, Xiangbo Shu, and Qi Tian<br>
			IEEE T-PAMI, 2020<br>
			[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9241410">PDF</a>][<a href="https://github.com/ruiyan1995/HiGCIN">Code</a>]
	 	</div>
	</div>

	<div class="publication media paperhi">
           <img src="img/SAM.png" height="100" width="200" class="papericon">
           <div class="media-body"><b>Social Adaptive Module for Weakly-supervised Group Activity Recognition</b><br>
           		<b>Rui Yan</b>, Lingxi Xie, Jinhui Tang, Xiangbo Shu, and Qi Tian<br>
           		ECCV 2020 <br>
           		[<a href="https://arxiv.org/pdf/2007.09470">PDF</a>][<a href="SAM.html">Project</a>][<a href="https://github.com/ruiyan1995/Weakly-supervised-Group-Activiy-Recognition">Code</a>]
			</div>
	</div>


	<!-- <div class="publication media paperhi">
           <img src="img/SRM.png" height="100" width="200" class="papericon">
           <div class="media-body"><b>Storyboard Region Relationship Model for Group Activity Recognition</b><br>
           		Boning Li, Xiangbo Shu, and <b>Rui Yan</b><br>
           		ACM MM Asia 2020<br>
           		[<a href="xxx">PDF</a>]
			</div>
	</div> -->


	<div class="publication media paperhi">
           <img src="img/CCGL.png" height="100" width="200" class="papericon">
           <div class="media-body"><b>Coherence Constrained Graph LSTM for Group Activity Recognition</b><br>
           		Jinhui Tang, Xiangbo Shu, <b>Rui Yan</b>, and Liyan Zhang<br>
           		IEEE T-PAMI, 2019<br>
           		[<a href="xxx">PDF</a>]
			</div>
	</div>

	<div class="publication media paperhi">
           <img src="img/PC-TDM.jpg" height="100" width="200" class="papericon">
           <div class="media-body"><b>Participation-Contributed Temporal Dynamic Model for Group Activity Recognition</b><br>
           		<b>Rui Yan</b>, Jinhui Tang, Xiangbo Shu, Zechao Li and Qi Tian<br>
           		ACM MM 2018 (Oral) ~8.5% (Journal version is accepted by TNNLS)<br>
           		[<a href="https://www.researchgate.net/profile/Rui_Yan31/publication/328372578_Participation-Contributed_Temporal_Dynamic_Model_for_Group_Activity_Recognition/links/5bed27684585150b2bb79e69/Participation-Contributed-Temporal-Dynamic-Model-for-Group-Activity-Recognition.pdf">PDF</a>][<a href="https://github.com/ruiyan1995/Group-Activity-Recognition">Code</a>][<a href="https://github.com/ruiyan1995/ruiyan1995.github.io/raw/master/mm2018_korea.pptx">Slides</a>]
			</div>
	</div>

	

	<!-- <div class="publication media paperhi">
           <img src="img/Skip-Attention.jpg" height="100" width="200" class="papericon">
           <div class="media-body"><b>Skip-Attention Encoder-Decoder Framework for Human Motion Prediction</b><br>
           		Ruipeng Zhang, Xiangbo Shu, <b>Rui Yan</b>, Jiachao Zhang and Yan Song<br>
           		China MM 2020 (recommended to Multimedia Systems)<br>
           		[<a href="">PDF</a>]
			</div>
	</div> -->

	<!-- <div class="publication media paperhi">
           <img src="img/none.jpeg" height="100" width="200" class="papericon">
           <div class="media-body"><b>A Feature Selection Method for Projection Twin Support Vector Machine</b><br>
           		<b>Rui Yan</b>, Qiaolin Ye, Liyan Zhang, Ning Ye and Xiangbo Shu<br>
           		Neural Processing Letters, 2016<br>
           		[<a href="https://www.researchgate.net/profile/Rui_Yan31/publication/317769500_A_Feature_Selection_Method_for_Projection_Twin_Support_Vector_Machine/links/59b1370d458515a5b4890247/A-Feature-Selection-Method-for-Projection-Twin-Support-Vector-Machine.pdf">PDF</a>]
			</div>
	</div> -->



</div>
    <h2>Honors</h2>
	<div>
        <ul>
        	<li>The Outstanding PhD of Nanjing University of Science and Technology 2020</li>
        	<li>ACM Multimedia Student Travel Grant 2018</li>
        	<li>The First Prize Scholarship of Nanjing University of Science and Technology 2017, 2018</li>
            <li>Excellent Undergraduate Thesis of Jiangsu Province 2018</li>
            <li>Top Ten Outstanding Youth of Nanjing Forestry University 2017</li>
            <li>The Second prize of National University Students Computer Design Competition 2016</li>
            <li>The National Encouragement Scholarship 2014, 2015, 2016</li>
            <li>Merit Student of Nanjing Forestry University 2014, 2015, 2016</li>
        </ul>    
	</div>

	<h2>Professional Services</h2>
	<div>
		<ul>
			<li>
				TPC for IEEE MIPR 2020 and BigMM 2021.
			</li>
			<li>
				Reviewer for Information Sciences, TIP, TNNLS, TCSVT, PR, Neurocomputing, Multimedia Systems, The Visual Computer, KSII Transactions on Internet and Information Systems.
			</li>
			<li>
				Reviewer for CVPR 2022, ICCV 2021, IJCNN 2021, PCM 2018, ICMBD 2018.
			</li>
		</ul>
	</div>
</div>

</div>
<a href='http://bzhou.ie.cuhk.edu.hk/'>Source stolen from here</a>
</body></html>
